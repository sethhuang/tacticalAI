{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yfinance pytorch-lightning pytorch_tabnet\n",
    "# pip install deepforest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import talib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "symbol = 'META'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2022-01-01'\n",
    "\n",
    "data = yf.download(symbol, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-05-18</th>\n",
       "      <td>42.049999</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.230000</td>\n",
       "      <td>38.230000</td>\n",
       "      <td>573576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-21</th>\n",
       "      <td>36.529999</td>\n",
       "      <td>36.660000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.029999</td>\n",
       "      <td>34.029999</td>\n",
       "      <td>168192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-22</th>\n",
       "      <td>32.610001</td>\n",
       "      <td>33.590000</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>101786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-23</th>\n",
       "      <td>31.370001</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>31.360001</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>73600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-24</th>\n",
       "      <td>32.950001</td>\n",
       "      <td>33.209999</td>\n",
       "      <td>31.770000</td>\n",
       "      <td>33.029999</td>\n",
       "      <td>33.029999</td>\n",
       "      <td>50237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>338.850006</td>\n",
       "      <td>347.869995</td>\n",
       "      <td>338.010010</td>\n",
       "      <td>346.179993</td>\n",
       "      <td>346.179993</td>\n",
       "      <td>17795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>346.630005</td>\n",
       "      <td>352.709991</td>\n",
       "      <td>345.200012</td>\n",
       "      <td>346.220001</td>\n",
       "      <td>346.220001</td>\n",
       "      <td>16637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>346.910004</td>\n",
       "      <td>349.690002</td>\n",
       "      <td>341.640015</td>\n",
       "      <td>342.940002</td>\n",
       "      <td>342.940002</td>\n",
       "      <td>10747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>347.230011</td>\n",
       "      <td>343.220001</td>\n",
       "      <td>344.359985</td>\n",
       "      <td>344.359985</td>\n",
       "      <td>10593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>343.019989</td>\n",
       "      <td>343.440002</td>\n",
       "      <td>336.269989</td>\n",
       "      <td>336.350006</td>\n",
       "      <td>336.350006</td>\n",
       "      <td>12870500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2422 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2012-05-18   42.049999   45.000000   38.000000   38.230000   38.230000   \n",
       "2012-05-21   36.529999   36.660000   33.000000   34.029999   34.029999   \n",
       "2012-05-22   32.610001   33.590000   30.940001   31.000000   31.000000   \n",
       "2012-05-23   31.370001   32.500000   31.360001   32.000000   32.000000   \n",
       "2012-05-24   32.950001   33.209999   31.770000   33.029999   33.029999   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2021-12-27  338.850006  347.869995  338.010010  346.179993  346.179993   \n",
       "2021-12-28  346.630005  352.709991  345.200012  346.220001  346.220001   \n",
       "2021-12-29  346.910004  349.690002  341.640015  342.940002  342.940002   \n",
       "2021-12-30  344.000000  347.230011  343.220001  344.359985  344.359985   \n",
       "2021-12-31  343.019989  343.440002  336.269989  336.350006  336.350006   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2012-05-18  573576400  \n",
       "2012-05-21  168192700  \n",
       "2012-05-22  101786600  \n",
       "2012-05-23   73600000  \n",
       "2012-05-24   50237200  \n",
       "...               ...  \n",
       "2021-12-27   17795000  \n",
       "2021-12-28   16637600  \n",
       "2021-12-29   10747000  \n",
       "2021-12-30   10593300  \n",
       "2021-12-31   12870500  \n",
       "\n",
       "[2422 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the target variable (e.g., whether the stock price increased the next day)\n",
    "data['target'] = np.where(data['Adj Close'].shift(-1) > data['Adj Close'], 1, 0)\n",
    "\n",
    "# Drop the last row, which has no target value\n",
    "data = data[:-1]\n",
    "\n",
    "# Use technical indicators or any other feature engineering you'd like to perform\n",
    "\n",
    "# Normalize the data (excluding the target variable)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['target']))\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns[:-1], index=data.index)\n",
    "\n",
    "# Add the target variable back\n",
    "data_scaled['target'] = data['target'].values\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "X_train = train_data.drop(columns=['target']).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_test = test_data.drop(columns=['target']).values\n",
    "y_test = test_data['target'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['RSI'] = talib.RSI(data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['SMA_10'] = talib.SMA(data['Adj Close'], timeperiod=10)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['SMA_30'] = talib.SMA(data['Adj Close'], timeperiod=30)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['EMA_10'] = talib.EMA(data['Adj Close'], timeperiod=10)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['EMA_30'] = talib.EMA(data['Adj Close'], timeperiod=30)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['MACD'], data['MACD_signal'], _ = talib.MACD(data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['UpperBB'], data['MiddleBB'], data['LowerBB'] = talib.BBANDS(data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ADX'] = talib.ADX(data['High'], data['Low'], data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['CCI'] = talib.CCI(data['High'], data['Low'], data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ATR'] = talib.ATR(data['High'], data['Low'], data['Adj Close'])\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['SMA_cross'] = np.where(data['SMA_10'] > data['SMA_30'], 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['EMA_cross'] = np.where(data['EMA_10'] > data['EMA_30'], 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['MACD_cross'] = np.where(data['MACD'] > data['MACD_signal'], 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['BB_squeeze'] = np.where((data['UpperBB'] - data['LowerBB']) / data['MiddleBB'] < 0.05, 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ADX_trend'] = np.where(data['ADX'] > 25, 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['CCI_overbought'] = np.where(data['CCI'] > 100, 1, 0)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_83832/3241144021.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['CCI_oversold'] = np.where(data['CCI'] < -100, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "import talib\n",
    "\n",
    "# Add technical indicators\n",
    "data['RSI'] = talib.RSI(data['Adj Close'])\n",
    "data['SMA_10'] = talib.SMA(data['Adj Close'], timeperiod=10)\n",
    "data['SMA_30'] = talib.SMA(data['Adj Close'], timeperiod=30)\n",
    "data['EMA_10'] = talib.EMA(data['Adj Close'], timeperiod=10)\n",
    "data['EMA_30'] = talib.EMA(data['Adj Close'], timeperiod=30)\n",
    "data['MACD'], data['MACD_signal'], _ = talib.MACD(data['Adj Close'])\n",
    "data['UpperBB'], data['MiddleBB'], data['LowerBB'] = talib.BBANDS(data['Adj Close'])\n",
    "data['ADX'] = talib.ADX(data['High'], data['Low'], data['Adj Close'])\n",
    "data['CCI'] = talib.CCI(data['High'], data['Low'], data['Adj Close'])\n",
    "data['ATR'] = talib.ATR(data['High'], data['Low'], data['Adj Close'])\n",
    "\n",
    "# Create flags for important points\n",
    "data['SMA_cross'] = np.where(data['SMA_10'] > data['SMA_30'], 1, 0)\n",
    "data['EMA_cross'] = np.where(data['EMA_10'] > data['EMA_30'], 1, 0)\n",
    "data['MACD_cross'] = np.where(data['MACD'] > data['MACD_signal'], 1, 0)\n",
    "data['BB_squeeze'] = np.where((data['UpperBB'] - data['LowerBB']) / data['MiddleBB'] < 0.05, 1, 0)\n",
    "data['ADX_trend'] = np.where(data['ADX'] > 25, 1, 0)\n",
    "data['CCI_overbought'] = np.where(data['CCI'] > 100, 1, 0)\n",
    "data['CCI_oversold'] = np.where(data['CCI'] < -100, 1, 0)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['target']))\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns[:-1], index=data.index)\n",
    "\n",
    "# Add the target variable back\n",
    "data_scaled['target'] = data['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Prepare the data\n",
    "X_train = train_data.drop(columns=['target']).values\n",
    "y_train = train_data['target'].values\n",
    "X_test = test_data.drop(columns=['target']).values\n",
    "y_test = test_data['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75535 | val_0_accuracy: 0.51805 |  0:00:02s\n",
      "epoch 1  | loss: 0.70121 | val_0_accuracy: 0.46497 |  0:00:05s\n",
      "epoch 2  | loss: 0.69414 | val_0_accuracy: 0.53079 |  0:00:08s\n",
      "epoch 3  | loss: 0.69496 | val_0_accuracy: 0.49894 |  0:00:11s\n",
      "epoch 4  | loss: 0.69309 | val_0_accuracy: 0.50531 |  0:00:13s\n",
      "epoch 5  | loss: 0.69222 | val_0_accuracy: 0.52654 |  0:00:16s\n",
      "epoch 6  | loss: 0.69284 | val_0_accuracy: 0.54352 |  0:00:19s\n",
      "epoch 7  | loss: 0.69466 | val_0_accuracy: 0.48408 |  0:00:22s\n",
      "epoch 8  | loss: 0.6927  | val_0_accuracy: 0.49894 |  0:00:25s\n",
      "epoch 9  | loss: 0.69083 | val_0_accuracy: 0.50955 |  0:00:28s\n",
      "epoch 10 | loss: 0.6909  | val_0_accuracy: 0.52442 |  0:00:31s\n",
      "epoch 11 | loss: 0.6897  | val_0_accuracy: 0.53291 |  0:00:34s\n",
      "epoch 12 | loss: 0.69162 | val_0_accuracy: 0.54777 |  0:00:36s\n",
      "epoch 13 | loss: 0.69184 | val_0_accuracy: 0.53503 |  0:00:39s\n",
      "epoch 14 | loss: 0.69003 | val_0_accuracy: 0.5138  |  0:00:42s\n",
      "epoch 15 | loss: 0.69082 | val_0_accuracy: 0.49894 |  0:00:45s\n",
      "epoch 16 | loss: 0.6912  | val_0_accuracy: 0.48832 |  0:00:48s\n",
      "epoch 17 | loss: 0.69007 | val_0_accuracy: 0.56688 |  0:00:51s\n",
      "epoch 18 | loss: 0.69188 | val_0_accuracy: 0.53503 |  0:00:53s\n",
      "epoch 19 | loss: 0.68936 | val_0_accuracy: 0.47983 |  0:00:56s\n",
      "epoch 20 | loss: 0.69186 | val_0_accuracy: 0.57325 |  0:00:59s\n",
      "epoch 21 | loss: 0.69025 | val_0_accuracy: 0.58174 |  0:01:02s\n",
      "epoch 22 | loss: 0.69    | val_0_accuracy: 0.5414  |  0:01:04s\n",
      "epoch 23 | loss: 0.69024 | val_0_accuracy: 0.56263 |  0:01:07s\n",
      "epoch 24 | loss: 0.69039 | val_0_accuracy: 0.53291 |  0:01:10s\n",
      "epoch 25 | loss: 0.69016 | val_0_accuracy: 0.53079 |  0:01:13s\n",
      "epoch 26 | loss: 0.69097 | val_0_accuracy: 0.53503 |  0:01:15s\n",
      "epoch 27 | loss: 0.69162 | val_0_accuracy: 0.55839 |  0:01:18s\n",
      "epoch 28 | loss: 0.69086 | val_0_accuracy: 0.54565 |  0:01:21s\n",
      "epoch 29 | loss: 0.69068 | val_0_accuracy: 0.5138  |  0:01:24s\n",
      "epoch 30 | loss: 0.69154 | val_0_accuracy: 0.50318 |  0:01:26s\n",
      "epoch 31 | loss: 0.68896 | val_0_accuracy: 0.50531 |  0:01:29s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_accuracy = 0.58174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Precision (Long): 0.5755627009646302\n",
      "TabNet Precision (Short): 0.59375\n"
     ]
    }
   ],
   "source": [
    "# Train the TabNet model\n",
    "model = TabNetClassifier(seed=42)\n",
    "model.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=32,\n",
    "    virtual_batch_size=8,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Obtain probabilistic outputs\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long = precision_score(y_test, y_pred, pos_label=1)\n",
    "precision_short = precision_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "print(\"TabNet Precision (Long):\", precision_long)\n",
    "print(\"TabNet Precision (Short):\", precision_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Precision (Long): 0.40476190476190477\n",
      "Random Forest Precision (Short): 0.47086247086247085\n",
      "XGBoost Precision (Long): 0.536723163841808\n",
      "XGBoost Precision (Short): 0.4931972789115646\n",
      "CatBoost Precision (Long): 0.5328467153284672\n",
      "CatBoost Precision (Short): 0.5025380710659898\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Random Forest, XGBoost, and CatBoost classifiers\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "catboost = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "classifiers = [('Random Forest', rf), ('XGBoost', xgb), ('CatBoost', catboost)]\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    precision_long = precision_score(y_test, y_pred, pos_label=1)\n",
    "    precision_short = precision_score(y_test, y_pred, pos_label=0)\n",
    "    \n",
    "    print(f\"{name} Precision (Long):\", precision_long)\n",
    "    print(f\"{name} Precision (Short):\", precision_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble based on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Precision (Long): 0.5514018691588785\n",
      "Ensemble Precision (Short): 0.5097276264591439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Obtain probabilistic outputs for each model\n",
    "y_proba_tabnet = model.predict_proba(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)\n",
    "y_proba_catboost = catboost.predict_proba(X_test)\n",
    "\n",
    "# Calculate the average predicted probabilities\n",
    "y_proba_ensemble = (y_proba_tabnet + y_proba_rf + y_proba_xgb + y_proba_catboost) / 4\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred_ensemble = np.argmax(y_proba_ensemble, axis=1)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=1)\n",
    "precision_short_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=0)\n",
    "\n",
    "print(\"Ensemble Precision (Long):\", precision_long_ensemble)\n",
    "print(\"Ensemble Precision (Short):\", precision_short_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble based on Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble Precision (Long): 0.5808823529411765\n",
      "Voting Ensemble Precision (Short): 0.5074626865671642\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Obtain predicted classes for each model\n",
    "y_pred_tabnet = model.predict(X_test)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_pred_catboost = catboost.predict(X_test)\n",
    "\n",
    "# Combine the predictions into a single array\n",
    "predictions = np.vstack((y_pred_tabnet, y_pred_rf, y_pred_xgb, y_pred_catboost))\n",
    "\n",
    "# Calculate the mode (voting) for each instance\n",
    "y_pred_voting, _ = mode(predictions, axis=0)\n",
    "\n",
    "# Flatten the resulting array\n",
    "y_pred_voting = y_pred_voting.flatten()\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_voting = precision_score(y_test, y_pred_voting, pos_label=1)\n",
    "precision_short_voting = precision_score(y_test, y_pred_voting, pos_label=0)\n",
    "\n",
    "print(\"Voting Ensemble Precision (Long):\", precision_long_voting)\n",
    "print(\"Voting Ensemble Precision (Short):\", precision_short_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multiple TabNet models and get ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73198 | val_0_accuracy: 0.47771 |  0:00:02s\n",
      "epoch 1  | loss: 0.69913 | val_0_accuracy: 0.51805 |  0:00:05s\n",
      "epoch 2  | loss: 0.69358 | val_0_accuracy: 0.51805 |  0:00:08s\n",
      "epoch 3  | loss: 0.69498 | val_0_accuracy: 0.51805 |  0:00:11s\n",
      "epoch 4  | loss: 0.69412 | val_0_accuracy: 0.51805 |  0:00:13s\n",
      "epoch 5  | loss: 0.69204 | val_0_accuracy: 0.51805 |  0:00:16s\n",
      "epoch 6  | loss: 0.69214 | val_0_accuracy: 0.48832 |  0:00:19s\n",
      "epoch 7  | loss: 0.6927  | val_0_accuracy: 0.51592 |  0:00:22s\n",
      "epoch 8  | loss: 0.69198 | val_0_accuracy: 0.52017 |  0:00:24s\n",
      "epoch 9  | loss: 0.69316 | val_0_accuracy: 0.52654 |  0:00:27s\n",
      "epoch 10 | loss: 0.69232 | val_0_accuracy: 0.51805 |  0:00:30s\n",
      "epoch 11 | loss: 0.69227 | val_0_accuracy: 0.51805 |  0:00:32s\n",
      "epoch 12 | loss: 0.69392 | val_0_accuracy: 0.51805 |  0:00:35s\n",
      "epoch 13 | loss: 0.69268 | val_0_accuracy: 0.51805 |  0:00:38s\n",
      "epoch 14 | loss: 0.69036 | val_0_accuracy: 0.52017 |  0:00:40s\n",
      "epoch 15 | loss: 0.69109 | val_0_accuracy: 0.51805 |  0:00:43s\n",
      "epoch 16 | loss: 0.68926 | val_0_accuracy: 0.52017 |  0:00:46s\n",
      "epoch 17 | loss: 0.69115 | val_0_accuracy: 0.51805 |  0:00:49s\n",
      "epoch 18 | loss: 0.69185 | val_0_accuracy: 0.52654 |  0:00:51s\n",
      "epoch 19 | loss: 0.69214 | val_0_accuracy: 0.51805 |  0:00:54s\n",
      "epoch 20 | loss: 0.69238 | val_0_accuracy: 0.52442 |  0:00:57s\n",
      "epoch 21 | loss: 0.69057 | val_0_accuracy: 0.4862  |  0:00:59s\n",
      "epoch 22 | loss: 0.69315 | val_0_accuracy: 0.53291 |  0:01:02s\n",
      "epoch 23 | loss: 0.69025 | val_0_accuracy: 0.5138  |  0:01:05s\n",
      "epoch 24 | loss: 0.69112 | val_0_accuracy: 0.49045 |  0:01:07s\n",
      "epoch 25 | loss: 0.69203 | val_0_accuracy: 0.48195 |  0:01:10s\n",
      "epoch 26 | loss: 0.69118 | val_0_accuracy: 0.52654 |  0:01:13s\n",
      "epoch 27 | loss: 0.68845 | val_0_accuracy: 0.48832 |  0:01:16s\n",
      "epoch 28 | loss: 0.69104 | val_0_accuracy: 0.49682 |  0:01:18s\n",
      "epoch 29 | loss: 0.68781 | val_0_accuracy: 0.49257 |  0:01:21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7415  | val_0_accuracy: 0.52654 |  0:00:02s\n",
      "epoch 1  | loss: 0.69633 | val_0_accuracy: 0.53715 |  0:00:05s\n",
      "epoch 2  | loss: 0.69521 | val_0_accuracy: 0.53079 |  0:00:08s\n",
      "epoch 3  | loss: 0.69463 | val_0_accuracy: 0.54989 |  0:00:11s\n",
      "epoch 4  | loss: 0.69158 | val_0_accuracy: 0.50531 |  0:00:14s\n",
      "epoch 5  | loss: 0.69008 | val_0_accuracy: 0.53928 |  0:00:16s\n",
      "epoch 6  | loss: 0.69166 | val_0_accuracy: 0.46921 |  0:00:19s\n",
      "epoch 7  | loss: 0.69262 | val_0_accuracy: 0.51592 |  0:00:22s\n",
      "epoch 8  | loss: 0.6925  | val_0_accuracy: 0.50318 |  0:00:25s\n",
      "epoch 9  | loss: 0.69228 | val_0_accuracy: 0.50106 |  0:00:27s\n",
      "epoch 10 | loss: 0.69201 | val_0_accuracy: 0.50743 |  0:00:30s\n",
      "epoch 11 | loss: 0.69125 | val_0_accuracy: 0.5138  |  0:00:33s\n",
      "epoch 12 | loss: 0.68936 | val_0_accuracy: 0.50531 |  0:00:36s\n",
      "epoch 13 | loss: 0.69069 | val_0_accuracy: 0.51592 |  0:00:38s\n",
      "epoch 14 | loss: 0.69114 | val_0_accuracy: 0.50531 |  0:00:41s\n",
      "epoch 15 | loss: 0.6901  | val_0_accuracy: 0.52442 |  0:00:44s\n",
      "epoch 16 | loss: 0.69101 | val_0_accuracy: 0.51592 |  0:00:47s\n",
      "epoch 17 | loss: 0.68965 | val_0_accuracy: 0.51592 |  0:00:50s\n",
      "epoch 18 | loss: 0.68908 | val_0_accuracy: 0.50743 |  0:00:53s\n",
      "epoch 19 | loss: 0.69172 | val_0_accuracy: 0.52017 |  0:00:55s\n",
      "epoch 20 | loss: 0.69169 | val_0_accuracy: 0.5138  |  0:00:58s\n",
      "epoch 21 | loss: 0.69225 | val_0_accuracy: 0.51805 |  0:01:01s\n",
      "epoch 22 | loss: 0.69106 | val_0_accuracy: 0.52229 |  0:01:03s\n",
      "epoch 23 | loss: 0.69027 | val_0_accuracy: 0.52442 |  0:01:06s\n",
      "epoch 24 | loss: 0.6921  | val_0_accuracy: 0.54352 |  0:01:09s\n",
      "epoch 25 | loss: 0.69113 | val_0_accuracy: 0.52442 |  0:01:12s\n",
      "epoch 26 | loss: 0.69385 | val_0_accuracy: 0.52229 |  0:01:15s\n",
      "epoch 27 | loss: 0.6921  | val_0_accuracy: 0.52442 |  0:01:17s\n",
      "epoch 28 | loss: 0.69147 | val_0_accuracy: 0.51805 |  0:01:20s\n",
      "epoch 29 | loss: 0.69126 | val_0_accuracy: 0.50743 |  0:01:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75268 | val_0_accuracy: 0.52229 |  0:00:02s\n",
      "epoch 1  | loss: 0.69714 | val_0_accuracy: 0.48195 |  0:00:05s\n",
      "epoch 2  | loss: 0.69424 | val_0_accuracy: 0.52866 |  0:00:08s\n",
      "epoch 3  | loss: 0.69472 | val_0_accuracy: 0.51805 |  0:00:11s\n",
      "epoch 4  | loss: 0.69347 | val_0_accuracy: 0.52229 |  0:00:14s\n",
      "epoch 5  | loss: 0.69323 | val_0_accuracy: 0.51805 |  0:00:17s\n",
      "epoch 6  | loss: 0.69194 | val_0_accuracy: 0.50955 |  0:00:19s\n",
      "epoch 7  | loss: 0.6913  | val_0_accuracy: 0.5138  |  0:00:22s\n",
      "epoch 8  | loss: 0.69109 | val_0_accuracy: 0.48408 |  0:00:25s\n",
      "epoch 9  | loss: 0.692   | val_0_accuracy: 0.50743 |  0:00:28s\n",
      "epoch 10 | loss: 0.69065 | val_0_accuracy: 0.50955 |  0:00:31s\n",
      "epoch 11 | loss: 0.69052 | val_0_accuracy: 0.48195 |  0:00:34s\n",
      "epoch 12 | loss: 0.69125 | val_0_accuracy: 0.50106 |  0:00:36s\n",
      "epoch 13 | loss: 0.69172 | val_0_accuracy: 0.50318 |  0:00:39s\n",
      "epoch 14 | loss: 0.68983 | val_0_accuracy: 0.5138  |  0:00:42s\n",
      "epoch 15 | loss: 0.6907  | val_0_accuracy: 0.52442 |  0:00:45s\n",
      "epoch 16 | loss: 0.69259 | val_0_accuracy: 0.50531 |  0:00:48s\n",
      "epoch 17 | loss: 0.69117 | val_0_accuracy: 0.54565 |  0:00:51s\n",
      "epoch 18 | loss: 0.6928  | val_0_accuracy: 0.5138  |  0:00:54s\n",
      "epoch 19 | loss: 0.69205 | val_0_accuracy: 0.51592 |  0:00:57s\n",
      "epoch 20 | loss: 0.69185 | val_0_accuracy: 0.53715 |  0:01:00s\n",
      "epoch 21 | loss: 0.69132 | val_0_accuracy: 0.5138  |  0:01:03s\n",
      "epoch 22 | loss: 0.69272 | val_0_accuracy: 0.49257 |  0:01:06s\n",
      "epoch 23 | loss: 0.69214 | val_0_accuracy: 0.50318 |  0:01:09s\n",
      "epoch 24 | loss: 0.69205 | val_0_accuracy: 0.49469 |  0:01:12s\n",
      "epoch 25 | loss: 0.68998 | val_0_accuracy: 0.50743 |  0:01:15s\n",
      "epoch 26 | loss: 0.68805 | val_0_accuracy: 0.47771 |  0:01:18s\n",
      "epoch 27 | loss: 0.68766 | val_0_accuracy: 0.4862  |  0:01:21s\n",
      "epoch 28 | loss: 0.6907  | val_0_accuracy: 0.51168 |  0:01:25s\n",
      "epoch 29 | loss: 0.69171 | val_0_accuracy: 0.53079 |  0:01:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75742 | val_0_accuracy: 0.47771 |  0:00:02s\n",
      "epoch 1  | loss: 0.69578 | val_0_accuracy: 0.52229 |  0:00:05s\n",
      "epoch 2  | loss: 0.69131 | val_0_accuracy: 0.49257 |  0:00:08s\n",
      "epoch 3  | loss: 0.69242 | val_0_accuracy: 0.48195 |  0:00:10s\n",
      "epoch 4  | loss: 0.69245 | val_0_accuracy: 0.52654 |  0:00:13s\n",
      "epoch 5  | loss: 0.69014 | val_0_accuracy: 0.49469 |  0:00:16s\n",
      "epoch 6  | loss: 0.68974 | val_0_accuracy: 0.49894 |  0:00:20s\n",
      "epoch 7  | loss: 0.69174 | val_0_accuracy: 0.50106 |  0:00:23s\n",
      "epoch 8  | loss: 0.69124 | val_0_accuracy: 0.50531 |  0:00:25s\n",
      "epoch 9  | loss: 0.69016 | val_0_accuracy: 0.46285 |  0:00:28s\n",
      "epoch 10 | loss: 0.69006 | val_0_accuracy: 0.53291 |  0:00:31s\n",
      "epoch 11 | loss: 0.69277 | val_0_accuracy: 0.52866 |  0:00:34s\n",
      "epoch 12 | loss: 0.69077 | val_0_accuracy: 0.48408 |  0:00:36s\n",
      "epoch 13 | loss: 0.69223 | val_0_accuracy: 0.48832 |  0:00:39s\n",
      "epoch 14 | loss: 0.6916  | val_0_accuracy: 0.51168 |  0:00:42s\n",
      "epoch 15 | loss: 0.69236 | val_0_accuracy: 0.52017 |  0:00:45s\n",
      "epoch 16 | loss: 0.6914  | val_0_accuracy: 0.52654 |  0:00:47s\n",
      "epoch 17 | loss: 0.69071 | val_0_accuracy: 0.51592 |  0:00:50s\n",
      "epoch 18 | loss: 0.6904  | val_0_accuracy: 0.51805 |  0:00:53s\n",
      "epoch 19 | loss: 0.6916  | val_0_accuracy: 0.51805 |  0:00:56s\n",
      "epoch 20 | loss: 0.6909  | val_0_accuracy: 0.51805 |  0:00:59s\n",
      "epoch 21 | loss: 0.69328 | val_0_accuracy: 0.51592 |  0:01:01s\n",
      "epoch 22 | loss: 0.69028 | val_0_accuracy: 0.52017 |  0:01:04s\n",
      "epoch 23 | loss: 0.69351 | val_0_accuracy: 0.52229 |  0:01:07s\n",
      "epoch 24 | loss: 0.69163 | val_0_accuracy: 0.52654 |  0:01:10s\n",
      "epoch 25 | loss: 0.69116 | val_0_accuracy: 0.52442 |  0:01:12s\n",
      "epoch 26 | loss: 0.69085 | val_0_accuracy: 0.52017 |  0:01:15s\n",
      "epoch 27 | loss: 0.69161 | val_0_accuracy: 0.51805 |  0:01:18s\n",
      "epoch 28 | loss: 0.69227 | val_0_accuracy: 0.52017 |  0:01:21s\n",
      "epoch 29 | loss: 0.69161 | val_0_accuracy: 0.51805 |  0:01:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71359 | val_0_accuracy: 0.49894 |  0:00:02s\n",
      "epoch 1  | loss: 0.69482 | val_0_accuracy: 0.50318 |  0:00:05s\n",
      "epoch 2  | loss: 0.69433 | val_0_accuracy: 0.50743 |  0:00:08s\n",
      "epoch 3  | loss: 0.691   | val_0_accuracy: 0.52866 |  0:00:10s\n",
      "epoch 4  | loss: 0.69305 | val_0_accuracy: 0.50955 |  0:00:13s\n",
      "epoch 5  | loss: 0.69336 | val_0_accuracy: 0.5138  |  0:00:16s\n",
      "epoch 6  | loss: 0.69339 | val_0_accuracy: 0.51168 |  0:00:19s\n",
      "epoch 7  | loss: 0.69259 | val_0_accuracy: 0.54352 |  0:00:22s\n",
      "epoch 8  | loss: 0.69306 | val_0_accuracy: 0.51592 |  0:00:24s\n",
      "epoch 9  | loss: 0.69312 | val_0_accuracy: 0.52866 |  0:00:27s\n",
      "epoch 10 | loss: 0.69325 | val_0_accuracy: 0.51805 |  0:00:30s\n",
      "epoch 11 | loss: 0.69231 | val_0_accuracy: 0.51805 |  0:00:33s\n",
      "epoch 12 | loss: 0.69248 | val_0_accuracy: 0.51592 |  0:00:36s\n",
      "epoch 13 | loss: 0.69223 | val_0_accuracy: 0.49682 |  0:00:38s\n",
      "epoch 14 | loss: 0.69372 | val_0_accuracy: 0.52229 |  0:00:41s\n",
      "epoch 15 | loss: 0.69207 | val_0_accuracy: 0.51592 |  0:00:45s\n",
      "epoch 16 | loss: 0.69207 | val_0_accuracy: 0.5138  |  0:00:47s\n",
      "epoch 17 | loss: 0.68962 | val_0_accuracy: 0.51805 |  0:00:50s\n",
      "epoch 18 | loss: 0.69191 | val_0_accuracy: 0.49257 |  0:00:53s\n",
      "epoch 19 | loss: 0.6912  | val_0_accuracy: 0.51805 |  0:00:56s\n",
      "epoch 20 | loss: 0.69261 | val_0_accuracy: 0.49682 |  0:00:59s\n",
      "epoch 21 | loss: 0.69127 | val_0_accuracy: 0.52017 |  0:01:02s\n",
      "epoch 22 | loss: 0.6931  | val_0_accuracy: 0.49257 |  0:01:05s\n",
      "epoch 23 | loss: 0.69252 | val_0_accuracy: 0.51168 |  0:01:07s\n",
      "epoch 24 | loss: 0.69215 | val_0_accuracy: 0.54352 |  0:01:10s\n",
      "epoch 25 | loss: 0.69166 | val_0_accuracy: 0.50743 |  0:01:13s\n",
      "epoch 26 | loss: 0.69136 | val_0_accuracy: 0.49894 |  0:01:16s\n",
      "epoch 27 | loss: 0.69105 | val_0_accuracy: 0.51805 |  0:01:18s\n",
      "epoch 28 | loss: 0.69058 | val_0_accuracy: 0.48195 |  0:01:21s\n",
      "epoch 29 | loss: 0.69259 | val_0_accuracy: 0.49682 |  0:01:24s\n",
      "TabNet Ensemble Precision (Long): 0.5221674876847291\n",
      "TabNet Ensemble Precision (Short): 0.5076923076923077\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "N = 5  # number of TabNet models in the ensemble\n",
    "\n",
    "# Train N TabNet models with different random seeds\n",
    "tabnet_models = [TabNetClassifier(seed=i) for i in range(N)]\n",
    "\n",
    "for model in tabnet_models:\n",
    "    model.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=30,\n",
    "        patience=0,\n",
    "        batch_size=32,\n",
    "        virtual_batch_size=8,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "# Obtain the probabilistic outputs for each TabNet model\n",
    "y_probas = [model.predict_proba(X_test) for model in tabnet_models]\n",
    "\n",
    "# Calculate the average predicted probabilities\n",
    "y_proba_ensemble = np.mean(y_probas, axis=0)\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred_ensemble = np.argmax(y_proba_ensemble, axis=1)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=1)\n",
    "precision_short_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=0)\n",
    "\n",
    "print(\"TabNet Ensemble Precision (Long):\", precision_long_ensemble)\n",
    "print(\"TabNet Ensemble Precision (Short):\", precision_short_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Voting Ensemble Precision (Long): 0.5531914893617021\n",
      "TabNet Voting Ensemble Precision (Short): 0.49696969696969695\n"
     ]
    }
   ],
   "source": [
    "N = 5  # number of TabNet models in the ensemble\n",
    "vote_threshold = 0.8  # proportion of votes required to make a decision (e.g., 0.5 means at least half of the votes)\n",
    "\n",
    "\n",
    "# Obtain the predicted classes for each TabNet model\n",
    "y_preds = [model.predict(X_test) for model in tabnet_models]\n",
    "\n",
    "# Combine the predictions into a single array\n",
    "predictions = np.vstack(y_preds)\n",
    "\n",
    "# Calculate the proportion of votes for long position\n",
    "long_vote_proportion = np.mean(predictions, axis=0)\n",
    "\n",
    "# Make the final decision based on the vote_threshold\n",
    "y_pred_voting = np.where(long_vote_proportion >= vote_threshold, 1, 0)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_voting = precision_score(y_test, y_pred_voting, pos_label=1)\n",
    "precision_short_voting = precision_score(y_test, y_pred_voting, pos_label=0)\n",
    "\n",
    "print(\"TabNet Voting Ensemble Precision (Long):\", precision_long_voting)\n",
    "print(\"TabNet Voting Ensemble Precision (Short):\", precision_short_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TabNetClassifier, you can indeed adjust several hyperparameters to customize the model. Here are some commonly adjusted hyperparameters:\n",
    "\n",
    "n_d: The dimension of the decision prediction layer.\n",
    "\n",
    "n_a: The dimension of the attention prediction layer.\n",
    "\n",
    "n_steps: The total number of steps/decision layers in the TabNet model.\n",
    "\n",
    "gamma: The coefficient for feature sparsity loss.\n",
    "\n",
    "n_independent: The number of independent Gated Linear Unit (GLU) layers in each attention transformer.\n",
    "\n",
    "n_shared: The number of shared Gated Linear Unit (GLU) layers in each attention transformer.\n",
    "\n",
    "momentum: The momentum for batch normalization layers.\n",
    "\n",
    "optimizer_fn: The optimizer function used for training.\n",
    "\n",
    "scheduler_params: Parameters for the learning rate scheduler.\n",
    "\n",
    "scheduler_fn: The learning rate scheduler function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the same ensemble TabNet model with changed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73198 | val_0_accuracy: 0.47771 |  0:00:02s\n",
      "epoch 1  | loss: 0.69913 | val_0_accuracy: 0.51805 |  0:00:05s\n",
      "epoch 2  | loss: 0.69358 | val_0_accuracy: 0.51805 |  0:00:08s\n",
      "epoch 3  | loss: 0.69498 | val_0_accuracy: 0.51805 |  0:00:11s\n",
      "epoch 4  | loss: 0.69412 | val_0_accuracy: 0.51805 |  0:00:13s\n",
      "epoch 5  | loss: 0.69204 | val_0_accuracy: 0.51805 |  0:00:16s\n",
      "epoch 6  | loss: 0.69214 | val_0_accuracy: 0.48832 |  0:00:19s\n",
      "epoch 7  | loss: 0.6927  | val_0_accuracy: 0.51592 |  0:00:22s\n",
      "epoch 8  | loss: 0.69198 | val_0_accuracy: 0.52017 |  0:00:24s\n",
      "epoch 9  | loss: 0.69316 | val_0_accuracy: 0.52654 |  0:00:27s\n",
      "epoch 10 | loss: 0.69232 | val_0_accuracy: 0.51805 |  0:00:30s\n",
      "epoch 11 | loss: 0.69227 | val_0_accuracy: 0.51805 |  0:00:32s\n",
      "epoch 12 | loss: 0.69392 | val_0_accuracy: 0.51805 |  0:00:35s\n",
      "epoch 13 | loss: 0.69268 | val_0_accuracy: 0.51805 |  0:00:38s\n",
      "epoch 14 | loss: 0.69036 | val_0_accuracy: 0.52017 |  0:00:41s\n",
      "epoch 15 | loss: 0.69109 | val_0_accuracy: 0.51805 |  0:00:43s\n",
      "epoch 16 | loss: 0.68926 | val_0_accuracy: 0.52017 |  0:00:46s\n",
      "epoch 17 | loss: 0.69115 | val_0_accuracy: 0.51805 |  0:00:49s\n",
      "epoch 18 | loss: 0.69185 | val_0_accuracy: 0.52654 |  0:00:51s\n",
      "epoch 19 | loss: 0.69214 | val_0_accuracy: 0.51805 |  0:00:54s\n",
      "epoch 20 | loss: 0.69238 | val_0_accuracy: 0.52442 |  0:00:57s\n",
      "epoch 21 | loss: 0.69057 | val_0_accuracy: 0.4862  |  0:01:00s\n",
      "epoch 22 | loss: 0.69315 | val_0_accuracy: 0.53291 |  0:01:02s\n",
      "epoch 23 | loss: 0.69025 | val_0_accuracy: 0.5138  |  0:01:05s\n",
      "epoch 24 | loss: 0.69112 | val_0_accuracy: 0.49045 |  0:01:08s\n",
      "epoch 25 | loss: 0.69203 | val_0_accuracy: 0.48195 |  0:01:10s\n",
      "epoch 26 | loss: 0.69118 | val_0_accuracy: 0.52654 |  0:01:13s\n",
      "epoch 27 | loss: 0.68845 | val_0_accuracy: 0.48832 |  0:01:16s\n",
      "epoch 28 | loss: 0.69104 | val_0_accuracy: 0.49682 |  0:01:19s\n",
      "epoch 29 | loss: 0.68781 | val_0_accuracy: 0.49257 |  0:01:21s\n",
      "epoch 30 | loss: 0.69198 | val_0_accuracy: 0.51805 |  0:01:24s\n",
      "epoch 31 | loss: 0.68942 | val_0_accuracy: 0.51805 |  0:01:27s\n",
      "epoch 32 | loss: 0.68918 | val_0_accuracy: 0.50743 |  0:01:29s\n",
      "epoch 33 | loss: 0.68862 | val_0_accuracy: 0.52654 |  0:01:32s\n",
      "epoch 34 | loss: 0.68943 | val_0_accuracy: 0.52654 |  0:01:35s\n",
      "epoch 35 | loss: 0.68882 | val_0_accuracy: 0.51805 |  0:01:37s\n",
      "epoch 36 | loss: 0.69026 | val_0_accuracy: 0.51805 |  0:01:40s\n",
      "epoch 37 | loss: 0.69154 | val_0_accuracy: 0.51805 |  0:01:43s\n",
      "epoch 38 | loss: 0.68716 | val_0_accuracy: 0.52229 |  0:01:46s\n",
      "epoch 39 | loss: 0.68918 | val_0_accuracy: 0.52229 |  0:01:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7415  | val_0_accuracy: 0.52654 |  0:00:02s\n",
      "epoch 1  | loss: 0.69633 | val_0_accuracy: 0.53715 |  0:00:05s\n",
      "epoch 2  | loss: 0.69521 | val_0_accuracy: 0.53079 |  0:00:08s\n",
      "epoch 3  | loss: 0.69463 | val_0_accuracy: 0.54989 |  0:00:10s\n",
      "epoch 4  | loss: 0.69158 | val_0_accuracy: 0.50531 |  0:00:13s\n",
      "epoch 5  | loss: 0.69008 | val_0_accuracy: 0.53928 |  0:00:16s\n",
      "epoch 6  | loss: 0.69166 | val_0_accuracy: 0.46921 |  0:00:18s\n",
      "epoch 7  | loss: 0.69262 | val_0_accuracy: 0.51592 |  0:00:21s\n",
      "epoch 8  | loss: 0.6925  | val_0_accuracy: 0.50318 |  0:00:24s\n",
      "epoch 9  | loss: 0.69228 | val_0_accuracy: 0.50106 |  0:00:27s\n",
      "epoch 10 | loss: 0.69201 | val_0_accuracy: 0.50743 |  0:00:30s\n",
      "epoch 11 | loss: 0.69125 | val_0_accuracy: 0.5138  |  0:00:32s\n",
      "epoch 12 | loss: 0.68936 | val_0_accuracy: 0.50531 |  0:00:35s\n",
      "epoch 13 | loss: 0.69069 | val_0_accuracy: 0.51592 |  0:00:38s\n",
      "epoch 14 | loss: 0.69114 | val_0_accuracy: 0.50531 |  0:00:41s\n",
      "epoch 15 | loss: 0.6901  | val_0_accuracy: 0.52442 |  0:00:43s\n",
      "epoch 16 | loss: 0.69101 | val_0_accuracy: 0.51592 |  0:00:46s\n",
      "epoch 17 | loss: 0.68965 | val_0_accuracy: 0.51592 |  0:00:49s\n",
      "epoch 18 | loss: 0.68908 | val_0_accuracy: 0.50743 |  0:00:51s\n",
      "epoch 19 | loss: 0.69172 | val_0_accuracy: 0.52017 |  0:00:54s\n",
      "epoch 20 | loss: 0.69169 | val_0_accuracy: 0.5138  |  0:00:57s\n",
      "epoch 21 | loss: 0.69225 | val_0_accuracy: 0.51805 |  0:01:00s\n",
      "epoch 22 | loss: 0.69106 | val_0_accuracy: 0.52229 |  0:01:02s\n",
      "epoch 23 | loss: 0.69027 | val_0_accuracy: 0.52442 |  0:01:05s\n",
      "epoch 24 | loss: 0.6921  | val_0_accuracy: 0.54352 |  0:01:08s\n",
      "epoch 25 | loss: 0.69113 | val_0_accuracy: 0.52442 |  0:01:10s\n",
      "epoch 26 | loss: 0.69385 | val_0_accuracy: 0.52229 |  0:01:13s\n",
      "epoch 27 | loss: 0.6921  | val_0_accuracy: 0.52442 |  0:01:16s\n",
      "epoch 28 | loss: 0.69147 | val_0_accuracy: 0.51805 |  0:01:18s\n",
      "epoch 29 | loss: 0.69126 | val_0_accuracy: 0.50743 |  0:01:21s\n",
      "epoch 30 | loss: 0.69179 | val_0_accuracy: 0.5138  |  0:01:24s\n",
      "epoch 31 | loss: 0.69116 | val_0_accuracy: 0.50743 |  0:01:26s\n",
      "epoch 32 | loss: 0.69238 | val_0_accuracy: 0.51168 |  0:01:29s\n",
      "epoch 33 | loss: 0.6923  | val_0_accuracy: 0.51168 |  0:01:32s\n",
      "epoch 34 | loss: 0.69102 | val_0_accuracy: 0.5138  |  0:01:35s\n",
      "epoch 35 | loss: 0.69064 | val_0_accuracy: 0.49682 |  0:01:37s\n",
      "epoch 36 | loss: 0.69197 | val_0_accuracy: 0.52017 |  0:01:40s\n",
      "epoch 37 | loss: 0.69369 | val_0_accuracy: 0.49682 |  0:01:43s\n",
      "epoch 38 | loss: 0.69055 | val_0_accuracy: 0.53291 |  0:01:45s\n",
      "epoch 39 | loss: 0.68992 | val_0_accuracy: 0.52654 |  0:01:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75268 | val_0_accuracy: 0.52229 |  0:00:02s\n",
      "epoch 1  | loss: 0.69714 | val_0_accuracy: 0.48195 |  0:00:05s\n",
      "epoch 2  | loss: 0.69424 | val_0_accuracy: 0.52866 |  0:00:08s\n",
      "epoch 3  | loss: 0.69472 | val_0_accuracy: 0.51805 |  0:00:10s\n",
      "epoch 4  | loss: 0.69347 | val_0_accuracy: 0.52229 |  0:00:13s\n",
      "epoch 5  | loss: 0.69323 | val_0_accuracy: 0.51805 |  0:00:16s\n",
      "epoch 6  | loss: 0.69194 | val_0_accuracy: 0.50955 |  0:00:19s\n",
      "epoch 7  | loss: 0.6913  | val_0_accuracy: 0.5138  |  0:00:21s\n",
      "epoch 8  | loss: 0.69109 | val_0_accuracy: 0.48408 |  0:00:24s\n",
      "epoch 9  | loss: 0.692   | val_0_accuracy: 0.50743 |  0:00:27s\n",
      "epoch 10 | loss: 0.69065 | val_0_accuracy: 0.50955 |  0:00:29s\n",
      "epoch 11 | loss: 0.69052 | val_0_accuracy: 0.48195 |  0:00:32s\n",
      "epoch 12 | loss: 0.69125 | val_0_accuracy: 0.50106 |  0:00:35s\n",
      "epoch 13 | loss: 0.69172 | val_0_accuracy: 0.50318 |  0:00:37s\n",
      "epoch 14 | loss: 0.68983 | val_0_accuracy: 0.5138  |  0:00:40s\n",
      "epoch 15 | loss: 0.6907  | val_0_accuracy: 0.52442 |  0:00:43s\n",
      "epoch 16 | loss: 0.69259 | val_0_accuracy: 0.50531 |  0:00:46s\n",
      "epoch 17 | loss: 0.69117 | val_0_accuracy: 0.54565 |  0:00:48s\n",
      "epoch 18 | loss: 0.6928  | val_0_accuracy: 0.5138  |  0:00:51s\n",
      "epoch 19 | loss: 0.69205 | val_0_accuracy: 0.51592 |  0:00:54s\n",
      "epoch 20 | loss: 0.69185 | val_0_accuracy: 0.53715 |  0:00:56s\n",
      "epoch 21 | loss: 0.69132 | val_0_accuracy: 0.5138  |  0:00:59s\n",
      "epoch 22 | loss: 0.69272 | val_0_accuracy: 0.49257 |  0:01:02s\n",
      "epoch 23 | loss: 0.69214 | val_0_accuracy: 0.50318 |  0:01:05s\n",
      "epoch 24 | loss: 0.69205 | val_0_accuracy: 0.49469 |  0:01:07s\n",
      "epoch 25 | loss: 0.68998 | val_0_accuracy: 0.50743 |  0:01:10s\n",
      "epoch 26 | loss: 0.68805 | val_0_accuracy: 0.47771 |  0:01:13s\n",
      "epoch 27 | loss: 0.68766 | val_0_accuracy: 0.4862  |  0:01:16s\n",
      "epoch 28 | loss: 0.6907  | val_0_accuracy: 0.51168 |  0:01:18s\n",
      "epoch 29 | loss: 0.69171 | val_0_accuracy: 0.53079 |  0:01:21s\n",
      "epoch 30 | loss: 0.68931 | val_0_accuracy: 0.50531 |  0:01:24s\n",
      "epoch 31 | loss: 0.68739 | val_0_accuracy: 0.5138  |  0:01:26s\n",
      "epoch 32 | loss: 0.69034 | val_0_accuracy: 0.52229 |  0:01:29s\n",
      "epoch 33 | loss: 0.69074 | val_0_accuracy: 0.49469 |  0:01:32s\n",
      "epoch 34 | loss: 0.69251 | val_0_accuracy: 0.50531 |  0:01:34s\n",
      "epoch 35 | loss: 0.69099 | val_0_accuracy: 0.52866 |  0:01:37s\n",
      "epoch 36 | loss: 0.68943 | val_0_accuracy: 0.50955 |  0:01:40s\n",
      "epoch 37 | loss: 0.68943 | val_0_accuracy: 0.51805 |  0:01:43s\n",
      "epoch 38 | loss: 0.68939 | val_0_accuracy: 0.53928 |  0:01:45s\n",
      "epoch 39 | loss: 0.69298 | val_0_accuracy: 0.52442 |  0:01:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.75742 | val_0_accuracy: 0.47771 |  0:00:02s\n",
      "epoch 1  | loss: 0.69578 | val_0_accuracy: 0.52229 |  0:00:05s\n",
      "epoch 2  | loss: 0.69131 | val_0_accuracy: 0.49257 |  0:00:08s\n",
      "epoch 3  | loss: 0.69242 | val_0_accuracy: 0.48195 |  0:00:10s\n",
      "epoch 4  | loss: 0.69245 | val_0_accuracy: 0.52654 |  0:00:13s\n",
      "epoch 5  | loss: 0.69014 | val_0_accuracy: 0.49469 |  0:00:16s\n",
      "epoch 6  | loss: 0.68974 | val_0_accuracy: 0.49894 |  0:00:18s\n",
      "epoch 7  | loss: 0.69174 | val_0_accuracy: 0.50106 |  0:00:21s\n",
      "epoch 8  | loss: 0.69124 | val_0_accuracy: 0.50531 |  0:00:24s\n",
      "epoch 9  | loss: 0.69016 | val_0_accuracy: 0.46285 |  0:00:27s\n",
      "epoch 10 | loss: 0.69006 | val_0_accuracy: 0.53291 |  0:00:29s\n",
      "epoch 11 | loss: 0.69277 | val_0_accuracy: 0.52866 |  0:00:32s\n",
      "epoch 12 | loss: 0.69077 | val_0_accuracy: 0.48408 |  0:00:35s\n",
      "epoch 13 | loss: 0.69223 | val_0_accuracy: 0.48832 |  0:00:37s\n",
      "epoch 14 | loss: 0.6916  | val_0_accuracy: 0.51168 |  0:00:40s\n",
      "epoch 15 | loss: 0.69236 | val_0_accuracy: 0.52017 |  0:00:43s\n",
      "epoch 16 | loss: 0.6914  | val_0_accuracy: 0.52654 |  0:00:45s\n",
      "epoch 17 | loss: 0.69071 | val_0_accuracy: 0.51592 |  0:00:48s\n",
      "epoch 18 | loss: 0.6904  | val_0_accuracy: 0.51805 |  0:00:51s\n",
      "epoch 19 | loss: 0.6916  | val_0_accuracy: 0.51805 |  0:00:54s\n",
      "epoch 20 | loss: 0.6909  | val_0_accuracy: 0.51805 |  0:00:56s\n",
      "epoch 21 | loss: 0.69328 | val_0_accuracy: 0.51592 |  0:00:59s\n",
      "epoch 22 | loss: 0.69028 | val_0_accuracy: 0.52017 |  0:01:02s\n",
      "epoch 23 | loss: 0.69351 | val_0_accuracy: 0.52229 |  0:01:04s\n",
      "epoch 24 | loss: 0.69163 | val_0_accuracy: 0.52654 |  0:01:07s\n",
      "epoch 25 | loss: 0.69116 | val_0_accuracy: 0.52442 |  0:01:10s\n",
      "epoch 26 | loss: 0.69085 | val_0_accuracy: 0.52017 |  0:01:13s\n",
      "epoch 27 | loss: 0.69161 | val_0_accuracy: 0.51805 |  0:01:15s\n",
      "epoch 28 | loss: 0.69227 | val_0_accuracy: 0.52017 |  0:01:18s\n",
      "epoch 29 | loss: 0.69161 | val_0_accuracy: 0.51805 |  0:01:21s\n",
      "epoch 30 | loss: 0.69216 | val_0_accuracy: 0.51805 |  0:01:24s\n",
      "epoch 31 | loss: 0.6928  | val_0_accuracy: 0.51805 |  0:01:27s\n",
      "epoch 32 | loss: 0.691   | val_0_accuracy: 0.51805 |  0:01:30s\n",
      "epoch 33 | loss: 0.69164 | val_0_accuracy: 0.51805 |  0:01:33s\n",
      "epoch 34 | loss: 0.69089 | val_0_accuracy: 0.51805 |  0:01:36s\n",
      "epoch 35 | loss: 0.69135 | val_0_accuracy: 0.52017 |  0:01:39s\n",
      "epoch 36 | loss: 0.6913  | val_0_accuracy: 0.52229 |  0:01:42s\n",
      "epoch 37 | loss: 0.6906  | val_0_accuracy: 0.52866 |  0:01:45s\n",
      "epoch 38 | loss: 0.69291 | val_0_accuracy: 0.52229 |  0:01:48s\n",
      "epoch 39 | loss: 0.69097 | val_0_accuracy: 0.52442 |  0:01:51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71359 | val_0_accuracy: 0.49894 |  0:00:03s\n",
      "epoch 1  | loss: 0.69482 | val_0_accuracy: 0.50318 |  0:00:06s\n",
      "epoch 2  | loss: 0.69433 | val_0_accuracy: 0.50743 |  0:00:08s\n",
      "epoch 3  | loss: 0.691   | val_0_accuracy: 0.52866 |  0:00:11s\n",
      "epoch 4  | loss: 0.69305 | val_0_accuracy: 0.50955 |  0:00:14s\n",
      "epoch 5  | loss: 0.69336 | val_0_accuracy: 0.5138  |  0:00:16s\n",
      "epoch 6  | loss: 0.69339 | val_0_accuracy: 0.51168 |  0:00:19s\n",
      "epoch 7  | loss: 0.69259 | val_0_accuracy: 0.54352 |  0:00:22s\n",
      "epoch 8  | loss: 0.69306 | val_0_accuracy: 0.51592 |  0:00:25s\n",
      "epoch 9  | loss: 0.69312 | val_0_accuracy: 0.52866 |  0:00:27s\n",
      "epoch 10 | loss: 0.69325 | val_0_accuracy: 0.51805 |  0:00:30s\n",
      "epoch 11 | loss: 0.69231 | val_0_accuracy: 0.51805 |  0:00:33s\n",
      "epoch 12 | loss: 0.69248 | val_0_accuracy: 0.51592 |  0:00:35s\n",
      "epoch 13 | loss: 0.69223 | val_0_accuracy: 0.49682 |  0:00:38s\n",
      "epoch 14 | loss: 0.69372 | val_0_accuracy: 0.52229 |  0:00:41s\n",
      "epoch 15 | loss: 0.69207 | val_0_accuracy: 0.51592 |  0:00:44s\n",
      "epoch 16 | loss: 0.69207 | val_0_accuracy: 0.5138  |  0:00:46s\n",
      "epoch 17 | loss: 0.68962 | val_0_accuracy: 0.51805 |  0:00:49s\n",
      "epoch 18 | loss: 0.69191 | val_0_accuracy: 0.49257 |  0:00:52s\n",
      "epoch 19 | loss: 0.6912  | val_0_accuracy: 0.51805 |  0:00:54s\n",
      "epoch 20 | loss: 0.69261 | val_0_accuracy: 0.49682 |  0:00:57s\n",
      "epoch 21 | loss: 0.69127 | val_0_accuracy: 0.52017 |  0:01:00s\n",
      "epoch 22 | loss: 0.6931  | val_0_accuracy: 0.49257 |  0:01:02s\n",
      "epoch 23 | loss: 0.69252 | val_0_accuracy: 0.51168 |  0:01:05s\n",
      "epoch 24 | loss: 0.69215 | val_0_accuracy: 0.54352 |  0:01:08s\n",
      "epoch 25 | loss: 0.69166 | val_0_accuracy: 0.50743 |  0:01:11s\n",
      "epoch 26 | loss: 0.69136 | val_0_accuracy: 0.49894 |  0:01:13s\n",
      "epoch 27 | loss: 0.69105 | val_0_accuracy: 0.51805 |  0:01:16s\n",
      "epoch 28 | loss: 0.69058 | val_0_accuracy: 0.48195 |  0:01:19s\n",
      "epoch 29 | loss: 0.69259 | val_0_accuracy: 0.49682 |  0:01:21s\n",
      "epoch 30 | loss: 0.69136 | val_0_accuracy: 0.51805 |  0:01:24s\n",
      "epoch 31 | loss: 0.69189 | val_0_accuracy: 0.51168 |  0:01:27s\n",
      "epoch 32 | loss: 0.69141 | val_0_accuracy: 0.51805 |  0:01:29s\n",
      "epoch 33 | loss: 0.68997 | val_0_accuracy: 0.51805 |  0:01:32s\n",
      "epoch 34 | loss: 0.69157 | val_0_accuracy: 0.52442 |  0:01:35s\n",
      "epoch 35 | loss: 0.69208 | val_0_accuracy: 0.51805 |  0:01:38s\n",
      "epoch 36 | loss: 0.69168 | val_0_accuracy: 0.52017 |  0:01:40s\n",
      "epoch 37 | loss: 0.69215 | val_0_accuracy: 0.51805 |  0:01:43s\n",
      "epoch 38 | loss: 0.68922 | val_0_accuracy: 0.48408 |  0:01:46s\n",
      "epoch 39 | loss: 0.69193 | val_0_accuracy: 0.52654 |  0:01:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78169 | val_0_accuracy: 0.50743 |  0:00:02s\n",
      "epoch 1  | loss: 0.69561 | val_0_accuracy: 0.5138  |  0:00:05s\n",
      "epoch 2  | loss: 0.69442 | val_0_accuracy: 0.52442 |  0:00:08s\n",
      "epoch 3  | loss: 0.69418 | val_0_accuracy: 0.53291 |  0:00:10s\n",
      "epoch 4  | loss: 0.69355 | val_0_accuracy: 0.49682 |  0:00:13s\n",
      "epoch 5  | loss: 0.69239 | val_0_accuracy: 0.51805 |  0:00:16s\n",
      "epoch 6  | loss: 0.69313 | val_0_accuracy: 0.51805 |  0:00:18s\n",
      "epoch 7  | loss: 0.69284 | val_0_accuracy: 0.54565 |  0:00:21s\n",
      "epoch 8  | loss: 0.69344 | val_0_accuracy: 0.50955 |  0:00:24s\n",
      "epoch 9  | loss: 0.6922  | val_0_accuracy: 0.50106 |  0:00:27s\n",
      "epoch 10 | loss: 0.69255 | val_0_accuracy: 0.51592 |  0:00:29s\n",
      "epoch 11 | loss: 0.69109 | val_0_accuracy: 0.51805 |  0:00:32s\n",
      "epoch 12 | loss: 0.69239 | val_0_accuracy: 0.50743 |  0:00:35s\n",
      "epoch 13 | loss: 0.69244 | val_0_accuracy: 0.50318 |  0:00:37s\n",
      "epoch 14 | loss: 0.69194 | val_0_accuracy: 0.52654 |  0:00:40s\n",
      "epoch 15 | loss: 0.68931 | val_0_accuracy: 0.49682 |  0:00:43s\n",
      "epoch 16 | loss: 0.68998 | val_0_accuracy: 0.47558 |  0:00:46s\n",
      "epoch 17 | loss: 0.69267 | val_0_accuracy: 0.46497 |  0:00:48s\n",
      "epoch 18 | loss: 0.69209 | val_0_accuracy: 0.46921 |  0:00:51s\n",
      "epoch 19 | loss: 0.68968 | val_0_accuracy: 0.49469 |  0:00:54s\n",
      "epoch 20 | loss: 0.69    | val_0_accuracy: 0.50531 |  0:00:56s\n",
      "epoch 21 | loss: 0.69364 | val_0_accuracy: 0.51805 |  0:00:59s\n",
      "epoch 22 | loss: 0.69018 | val_0_accuracy: 0.52229 |  0:01:02s\n",
      "epoch 23 | loss: 0.69075 | val_0_accuracy: 0.49257 |  0:01:05s\n",
      "epoch 24 | loss: 0.69049 | val_0_accuracy: 0.49469 |  0:01:08s\n",
      "epoch 25 | loss: 0.68972 | val_0_accuracy: 0.50531 |  0:13:08s\n",
      "epoch 26 | loss: 0.68943 | val_0_accuracy: 0.50318 |  0:13:11s\n",
      "epoch 27 | loss: 0.68952 | val_0_accuracy: 0.50743 |  0:13:14s\n",
      "epoch 28 | loss: 0.69007 | val_0_accuracy: 0.50531 |  0:13:17s\n",
      "epoch 29 | loss: 0.6901  | val_0_accuracy: 0.52654 |  0:13:21s\n",
      "epoch 30 | loss: 0.68927 | val_0_accuracy: 0.52442 |  0:13:24s\n",
      "epoch 31 | loss: 0.68811 | val_0_accuracy: 0.50531 |  0:13:27s\n",
      "epoch 32 | loss: 0.68924 | val_0_accuracy: 0.5138  |  0:13:30s\n",
      "epoch 33 | loss: 0.68938 | val_0_accuracy: 0.52442 |  0:13:34s\n",
      "epoch 34 | loss: 0.69069 | val_0_accuracy: 0.50743 |  0:13:37s\n",
      "epoch 35 | loss: 0.68839 | val_0_accuracy: 0.5138  |  0:13:40s\n",
      "epoch 36 | loss: 0.68889 | val_0_accuracy: 0.49894 |  0:13:42s\n",
      "epoch 37 | loss: 0.68966 | val_0_accuracy: 0.48832 |  0:13:45s\n",
      "epoch 38 | loss: 0.68831 | val_0_accuracy: 0.49257 |  0:13:48s\n",
      "epoch 39 | loss: 0.69105 | val_0_accuracy: 0.50318 |  0:13:50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71319 | val_0_accuracy: 0.52017 |  0:00:02s\n",
      "epoch 1  | loss: 0.69574 | val_0_accuracy: 0.54565 |  0:00:05s\n",
      "epoch 2  | loss: 0.69645 | val_0_accuracy: 0.51805 |  0:00:08s\n",
      "epoch 3  | loss: 0.69445 | val_0_accuracy: 0.48195 |  0:00:10s\n",
      "epoch 4  | loss: 0.69414 | val_0_accuracy: 0.48195 |  0:00:13s\n",
      "epoch 5  | loss: 0.69417 | val_0_accuracy: 0.47983 |  0:00:16s\n",
      "epoch 6  | loss: 0.6929  | val_0_accuracy: 0.52866 |  0:00:19s\n",
      "epoch 7  | loss: 0.69228 | val_0_accuracy: 0.48832 |  0:00:21s\n",
      "epoch 8  | loss: 0.69171 | val_0_accuracy: 0.50106 |  0:00:24s\n",
      "epoch 9  | loss: 0.69173 | val_0_accuracy: 0.5138  |  0:00:27s\n",
      "epoch 10 | loss: 0.69103 | val_0_accuracy: 0.54777 |  0:00:30s\n",
      "epoch 11 | loss: 0.69166 | val_0_accuracy: 0.50318 |  0:00:32s\n",
      "epoch 12 | loss: 0.69058 | val_0_accuracy: 0.49469 |  0:00:35s\n",
      "epoch 13 | loss: 0.69073 | val_0_accuracy: 0.49045 |  0:00:38s\n",
      "epoch 14 | loss: 0.69095 | val_0_accuracy: 0.51805 |  0:00:41s\n",
      "epoch 15 | loss: 0.69024 | val_0_accuracy: 0.50955 |  0:00:44s\n",
      "epoch 16 | loss: 0.69007 | val_0_accuracy: 0.51805 |  0:00:47s\n",
      "epoch 17 | loss: 0.68939 | val_0_accuracy: 0.51805 |  0:00:50s\n",
      "epoch 18 | loss: 0.68958 | val_0_accuracy: 0.53715 |  0:00:52s\n",
      "epoch 19 | loss: 0.69072 | val_0_accuracy: 0.50743 |  0:00:55s\n",
      "epoch 20 | loss: 0.68966 | val_0_accuracy: 0.52017 |  0:00:58s\n",
      "epoch 21 | loss: 0.68828 | val_0_accuracy: 0.49257 |  0:01:01s\n",
      "epoch 22 | loss: 0.69178 | val_0_accuracy: 0.50955 |  0:01:03s\n",
      "epoch 23 | loss: 0.68889 | val_0_accuracy: 0.50531 |  0:01:06s\n",
      "epoch 24 | loss: 0.68889 | val_0_accuracy: 0.5138  |  0:01:09s\n",
      "epoch 25 | loss: 0.68937 | val_0_accuracy: 0.51805 |  0:01:12s\n",
      "epoch 26 | loss: 0.69064 | val_0_accuracy: 0.50531 |  0:01:15s\n",
      "epoch 27 | loss: 0.6892  | val_0_accuracy: 0.52017 |  0:01:18s\n",
      "epoch 28 | loss: 0.69075 | val_0_accuracy: 0.51592 |  0:01:21s\n",
      "epoch 29 | loss: 0.68881 | val_0_accuracy: 0.5414  |  0:01:24s\n",
      "epoch 30 | loss: 0.68961 | val_0_accuracy: 0.52442 |  0:01:27s\n",
      "epoch 31 | loss: 0.68945 | val_0_accuracy: 0.54352 |  0:01:30s\n",
      "epoch 32 | loss: 0.68974 | val_0_accuracy: 0.49682 |  0:01:33s\n",
      "epoch 33 | loss: 0.6916  | val_0_accuracy: 0.53928 |  0:01:36s\n",
      "epoch 34 | loss: 0.6902  | val_0_accuracy: 0.52654 |  0:01:39s\n",
      "epoch 35 | loss: 0.69177 | val_0_accuracy: 0.52442 |  0:01:42s\n",
      "epoch 36 | loss: 0.69197 | val_0_accuracy: 0.55839 |  0:01:45s\n",
      "epoch 37 | loss: 0.69067 | val_0_accuracy: 0.56688 |  0:01:48s\n",
      "epoch 38 | loss: 0.68813 | val_0_accuracy: 0.50955 |  0:01:51s\n",
      "epoch 39 | loss: 0.69169 | val_0_accuracy: 0.52017 |  0:01:54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:651: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.78663 | val_0_accuracy: 0.51168 |  0:00:03s\n",
      "epoch 1  | loss: 0.70133 | val_0_accuracy: 0.47558 |  0:00:06s\n",
      "epoch 2  | loss: 0.69311 | val_0_accuracy: 0.47346 |  0:00:09s\n",
      "epoch 3  | loss: 0.69459 | val_0_accuracy: 0.48408 |  0:00:12s\n",
      "epoch 4  | loss: 0.6933  | val_0_accuracy: 0.49469 |  0:00:15s\n",
      "epoch 5  | loss: 0.69114 | val_0_accuracy: 0.52866 |  0:00:18s\n",
      "epoch 6  | loss: 0.69076 | val_0_accuracy: 0.52229 |  0:00:21s\n",
      "epoch 7  | loss: 0.69155 | val_0_accuracy: 0.49045 |  0:00:24s\n",
      "epoch 8  | loss: 0.69172 | val_0_accuracy: 0.52654 |  0:00:27s\n",
      "epoch 9  | loss: 0.69248 | val_0_accuracy: 0.50318 |  0:00:30s\n",
      "epoch 10 | loss: 0.69162 | val_0_accuracy: 0.52017 |  0:00:33s\n",
      "epoch 11 | loss: 0.69176 | val_0_accuracy: 0.52017 |  0:00:36s\n",
      "epoch 12 | loss: 0.69264 | val_0_accuracy: 0.52442 |  0:00:39s\n",
      "epoch 13 | loss: 0.6922  | val_0_accuracy: 0.5138  |  0:00:42s\n",
      "epoch 14 | loss: 0.69257 | val_0_accuracy: 0.53291 |  0:00:45s\n",
      "epoch 15 | loss: 0.69183 | val_0_accuracy: 0.52229 |  0:00:48s\n",
      "epoch 16 | loss: 0.6923  | val_0_accuracy: 0.52229 |  0:00:51s\n",
      "epoch 17 | loss: 0.69205 | val_0_accuracy: 0.51592 |  0:00:54s\n",
      "epoch 18 | loss: 0.69199 | val_0_accuracy: 0.51592 |  0:00:57s\n",
      "epoch 19 | loss: 0.69276 | val_0_accuracy: 0.52654 |  0:01:00s\n",
      "epoch 20 | loss: 0.69292 | val_0_accuracy: 0.52654 |  0:01:03s\n",
      "epoch 21 | loss: 0.6922  | val_0_accuracy: 0.52229 |  0:01:07s\n",
      "epoch 22 | loss: 0.69171 | val_0_accuracy: 0.51805 |  0:01:10s\n",
      "epoch 23 | loss: 0.69084 | val_0_accuracy: 0.52866 |  0:01:13s\n",
      "epoch 24 | loss: 0.69023 | val_0_accuracy: 0.53291 |  0:01:16s\n",
      "epoch 25 | loss: 0.69181 | val_0_accuracy: 0.50955 |  0:01:19s\n",
      "epoch 26 | loss: 0.69146 | val_0_accuracy: 0.50955 |  0:01:22s\n",
      "epoch 27 | loss: 0.69157 | val_0_accuracy: 0.5138  |  0:01:24s\n",
      "epoch 28 | loss: 0.69103 | val_0_accuracy: 0.52229 |  0:01:27s\n",
      "epoch 29 | loss: 0.69033 | val_0_accuracy: 0.51592 |  0:01:30s\n",
      "epoch 30 | loss: 0.69    | val_0_accuracy: 0.50106 |  0:01:32s\n",
      "epoch 31 | loss: 0.68974 | val_0_accuracy: 0.52017 |  0:01:35s\n",
      "epoch 32 | loss: 0.69112 | val_0_accuracy: 0.47558 |  0:01:38s\n",
      "epoch 33 | loss: 0.69052 | val_0_accuracy: 0.49469 |  0:01:40s\n",
      "epoch 34 | loss: 0.68976 | val_0_accuracy: 0.50106 |  0:01:43s\n",
      "epoch 35 | loss: 0.68897 | val_0_accuracy: 0.50743 |  0:01:46s\n",
      "epoch 36 | loss: 0.68827 | val_0_accuracy: 0.52017 |  0:01:48s\n",
      "epoch 37 | loss: 0.6908  | val_0_accuracy: 0.49257 |  0:01:51s\n",
      "epoch 38 | loss: 0.69109 | val_0_accuracy: 0.4862  |  0:01:54s\n",
      "epoch 39 | loss: 0.68834 | val_0_accuracy: 0.5414  |  0:01:56s\n",
      "TabNet Ensemble Precision (Long): 0.541871921182266\n",
      "TabNet Ensemble Precision (Short): 0.6307692307692307\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "\n",
    "model = TabNetClassifier(\n",
    "    n_d=8,\n",
    "    n_a=8,\n",
    "    n_steps=3,\n",
    "    gamma=1.3,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    lambda_sparse=0.001,\n",
    "    seed=42,\n",
    "    optimizer_fn=torch.optim.Adam,  # You can also try other optimizers like AdamW or RMSprop\n",
    "    optimizer_params=dict(lr=0.02),  # You can experiment with different learning rates\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,  # Try using a learning rate scheduler\n",
    "    scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "\n",
    "N = 8  # number of TabNet models in the ensemble\n",
    "\n",
    "# Train N TabNet models with different random seeds\n",
    "tabnet_models = [TabNetClassifier(seed=i) for i in range(N)]\n",
    "\n",
    "for model in tabnet_models:\n",
    "    model.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=40,\n",
    "        patience=0,\n",
    "        batch_size=32,\n",
    "        virtual_batch_size=8,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet Averaging Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Ensemble Precision (Long): 0.541871921182266\n",
      "TabNet Ensemble Precision (Short): 0.6307692307692307\n"
     ]
    }
   ],
   "source": [
    "# Obtain the probabilistic outputs for each TabNet model\n",
    "y_probas = [model.predict_proba(X_test) for model in tabnet_models]\n",
    "\n",
    "# Calculate the average predicted probabilities\n",
    "y_proba_ensemble = np.mean(y_probas, axis=0)\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred_ensemble = np.argmax(y_proba_ensemble, axis=1)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=1)\n",
    "precision_short_ensemble = precision_score(y_test, y_pred_ensemble, pos_label=0)\n",
    "\n",
    "print(\"TabNet Ensemble Precision (Long):\", precision_long_ensemble)\n",
    "print(\"TabNet Ensemble Precision (Short):\", precision_short_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet Voting Ensemble Precision (Long): 0.5334821428571429\n",
      "TabNet Voting Ensemble Precision (Short): 0.782608695652174\n"
     ]
    }
   ],
   "source": [
    "N = 5  # number of TabNet models in the ensemble\n",
    "vote_threshold = 0.5  # proportion of votes required to make a decision (e.g., 0.5 means at least half of the votes)\n",
    "\n",
    "\n",
    "# Obtain the predicted classes for each TabNet model\n",
    "y_preds = [model.predict(X_test) for model in tabnet_models]\n",
    "\n",
    "# Combine the predictions into a single array\n",
    "predictions = np.vstack(y_preds)\n",
    "\n",
    "# Calculate the proportion of votes for long position\n",
    "long_vote_proportion = np.mean(predictions, axis=0)\n",
    "\n",
    "# Make the final decision based on the vote_threshold\n",
    "y_pred_voting = np.where(long_vote_proportion >= vote_threshold, 1, 0)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long_voting = precision_score(y_test, y_pred_voting, pos_label=1)\n",
    "precision_short_voting = precision_score(y_test, y_pred_voting, pos_label=0)\n",
    "\n",
    "print(\"TabNet Voting Ensemble Precision (Long):\", precision_long_voting)\n",
    "print(\"TabNet Voting Ensemble Precision (Short):\", precision_short_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Obtain probabilistic outputs\n",
    "y_proba = model.predict_proba(X_test)\n",
    "y_positive_proba = y_proba[:, 1]\n",
    "\n",
    "# Calculate precision and recall for various thresholds\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_positive_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9480.94931| val_0_mse: 18458.01593|  0:00:02s\n",
      "epoch 1  | loss: 564.79589| val_0_mse: 14347.35818|  0:00:05s\n",
      "epoch 2  | loss: 364.90291| val_0_mse: 13619.19137|  0:00:07s\n",
      "epoch 3  | loss: 359.35678| val_0_mse: 12131.37537|  0:00:10s\n",
      "epoch 4  | loss: 201.93088| val_0_mse: 6755.18367|  0:00:13s\n",
      "epoch 5  | loss: 209.66512| val_0_mse: 2712.92818|  0:00:15s\n",
      "epoch 6  | loss: 209.13014| val_0_mse: 5192.40784|  0:00:18s\n",
      "epoch 7  | loss: 184.00081| val_0_mse: 2206.992|  0:00:21s\n",
      "epoch 8  | loss: 196.27242| val_0_mse: 2635.81241|  0:00:23s\n",
      "epoch 9  | loss: 193.62599| val_0_mse: 1929.19247|  0:00:26s\n",
      "epoch 10 | loss: 183.46605| val_0_mse: 557.04726|  0:00:28s\n",
      "epoch 11 | loss: 171.75757| val_0_mse: 1103.42067|  0:00:31s\n",
      "epoch 12 | loss: 176.89312| val_0_mse: 110.15885|  0:00:34s\n",
      "epoch 13 | loss: 172.96622| val_0_mse: 1735.33021|  0:00:36s\n",
      "epoch 14 | loss: 175.40888| val_0_mse: 2245.30869|  0:00:39s\n",
      "epoch 15 | loss: 148.82792| val_0_mse: 2073.19852|  0:00:42s\n",
      "epoch 16 | loss: 126.99824| val_0_mse: 1362.68984|  0:00:44s\n",
      "epoch 17 | loss: 123.35462| val_0_mse: 2217.12625|  0:00:47s\n",
      "epoch 18 | loss: 163.09268| val_0_mse: 3364.31718|  0:00:49s\n",
      "epoch 19 | loss: 114.53424| val_0_mse: 1278.47118|  0:00:52s\n",
      "epoch 20 | loss: 111.87963| val_0_mse: 1538.3302|  0:00:55s\n",
      "epoch 21 | loss: 144.34653| val_0_mse: 3978.26576|  0:00:57s\n",
      "epoch 22 | loss: 121.40015| val_0_mse: 3001.38651|  0:01:00s\n",
      "epoch 23 | loss: 129.03959| val_0_mse: 597.58703|  0:01:03s\n",
      "epoch 24 | loss: 111.7363| val_0_mse: 1188.40034|  0:01:05s\n",
      "epoch 25 | loss: 141.59921| val_0_mse: 858.7532|  0:01:08s\n",
      "epoch 26 | loss: 161.09591| val_0_mse: 230.50097|  0:01:10s\n",
      "epoch 27 | loss: 119.6822| val_0_mse: 1246.27215|  0:01:13s\n",
      "epoch 28 | loss: 145.77976| val_0_mse: 646.5478|  0:01:16s\n",
      "epoch 29 | loss: 105.31611| val_0_mse: 1644.37525|  0:01:18s\n",
      "epoch 30 | loss: 125.77515| val_0_mse: 585.57018|  0:01:21s\n",
      "epoch 31 | loss: 121.10271| val_0_mse: 1513.76394|  0:01:23s\n",
      "epoch 32 | loss: 123.82401| val_0_mse: 476.46657|  0:01:26s\n",
      "epoch 33 | loss: 139.15869| val_0_mse: 1950.66155|  0:01:29s\n",
      "epoch 34 | loss: 115.9719| val_0_mse: 1434.87839|  0:01:31s\n",
      "epoch 35 | loss: 127.75738| val_0_mse: 655.41585|  0:01:34s\n",
      "epoch 36 | loss: 130.20114| val_0_mse: 716.72769|  0:01:37s\n",
      "epoch 37 | loss: 135.44587| val_0_mse: 512.51218|  0:01:39s\n",
      "epoch 38 | loss: 92.74443| val_0_mse: 1350.46256|  0:01:42s\n",
      "epoch 39 | loss: 119.65733| val_0_mse: 431.4179|  0:01:44s\n",
      "epoch 40 | loss: 106.23997| val_0_mse: 769.50677|  0:01:47s\n",
      "epoch 41 | loss: 120.4458| val_0_mse: 203.5258|  0:01:50s\n",
      "epoch 42 | loss: 117.56218| val_0_mse: 1654.68691|  0:01:52s\n",
      "epoch 43 | loss: 161.35821| val_0_mse: 495.17851|  0:01:55s\n",
      "epoch 44 | loss: 86.50212| val_0_mse: 653.51841|  0:01:58s\n",
      "epoch 45 | loss: 150.0736| val_0_mse: 2243.83819|  0:02:00s\n",
      "epoch 46 | loss: 97.43945| val_0_mse: 538.21317|  0:02:03s\n",
      "epoch 47 | loss: 104.26136| val_0_mse: 1014.80758|  0:02:05s\n",
      "epoch 48 | loss: 141.35693| val_0_mse: 1722.15293|  0:02:08s\n",
      "epoch 49 | loss: 117.98591| val_0_mse: 776.76478|  0:02:11s\n",
      "epoch 50 | loss: 107.87263| val_0_mse: 691.08641|  0:02:13s\n",
      "epoch 51 | loss: 104.81575| val_0_mse: 560.58551|  0:02:16s\n",
      "epoch 52 | loss: 109.44289| val_0_mse: 468.15451|  0:02:19s\n",
      "epoch 53 | loss: 123.62084| val_0_mse: 291.70942|  0:02:21s\n",
      "epoch 54 | loss: 118.10144| val_0_mse: 126.95514|  0:02:24s\n",
      "epoch 55 | loss: 112.86317| val_0_mse: 796.35362|  0:02:26s\n",
      "epoch 56 | loss: 149.70868| val_0_mse: 326.99455|  0:02:29s\n",
      "epoch 57 | loss: 114.89977| val_0_mse: 375.58412|  0:02:32s\n",
      "epoch 58 | loss: 122.11948| val_0_mse: 357.76466|  0:02:34s\n",
      "epoch 59 | loss: 89.25476| val_0_mse: 179.50469|  0:02:37s\n",
      "epoch 60 | loss: 115.84325| val_0_mse: 119.40725|  0:02:39s\n",
      "epoch 61 | loss: 111.35673| val_0_mse: 826.28974|  0:02:42s\n",
      "epoch 62 | loss: 120.35876| val_0_mse: 428.58176|  0:02:45s\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 12 and best_val_0_mse = 110.15885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cqm-primo/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download financial data using YFinance\n",
    "symbol = 'META'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2022-01-01'\n",
    "\n",
    "data = yf.download(symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate the target variable (percentage change in closing price)\n",
    "data['target'] = data['Close'].shift(-1)\n",
    "\n",
    "data['RSI'] = talib.RSI(data['Close'])\n",
    "data['SMA_10'] = talib.SMA(data['Close'], timeperiod=10)\n",
    "data['SMA_30'] = talib.SMA(data['Close'], timeperiod=30)\n",
    "data['EMA_10'] = talib.EMA(data['Close'], timeperiod=10)\n",
    "data['EMA_30'] = talib.EMA(data['Close'], timeperiod=30)\n",
    "data['MACD'], data['MACD_signal'], _ = talib.MACD(data['Close'])\n",
    "data['UpperBB'], data['MiddleBB'], data['LowerBB'] = talib.BBANDS(data['Close'])\n",
    "data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'])\n",
    "data['CCI'] = talib.CCI(data['High'], data['Low'], data['Close'])\n",
    "data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'])\n",
    "\n",
    "# Create flags for important points\n",
    "data['SMA_cross'] = np.where(data['SMA_10'] > data['SMA_30'], 1, 0)\n",
    "data['EMA_cross'] = np.where(data['EMA_10'] > data['EMA_30'], 1, 0)\n",
    "data['MACD_cross'] = np.where(data['MACD'] > data['MACD_signal'], 1, 0)\n",
    "data['BB_squeeze'] = np.where((data['UpperBB'] - data['LowerBB']) / data['MiddleBB'] < 0.05, 1, 0)\n",
    "data['ADX_trend'] = np.where(data['ADX'] > 25, 1, 0)\n",
    "data['CCI_overbought'] = np.where(data['CCI'] > 100, 1, 0)\n",
    "data['CCI_oversold'] = np.where(data['CCI'] < -100, 1, 0)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(columns=['target']))\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns[:-1], index=data.index)\n",
    "\n",
    "# Add the target variable back\n",
    "data_scaled['target'] = data['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Prepare the data\n",
    "X_train = train_data.drop(columns=['target']).values\n",
    "y_train = train_data['target'].values\n",
    "X_test = test_data.drop(columns=['target']).values\n",
    "y_test = test_data['target'].values\n",
    "\n",
    "# Train the TabNet model\n",
    "model = TabNetRegressor(seed=42)\n",
    "model.fit(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train.reshape(-1, 1),\n",
    "    eval_set=[(X_test, y_test.reshape(-1, 1))],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=200,\n",
    "    patience=50,\n",
    "    batch_size=32,\n",
    "    virtual_batch_size=8,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the true values and predictions\n",
    "plt.plot(y_test[0:50], label='True Values')\n",
    "plt.plot(y_pred[0:50], label='Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Percentage Change in Closing Price')\n",
    "plt.title('True Values vs Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for up predictions: 0.51\n",
      "Precision for down predictions: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate up and down prediction accuracy\n",
    "test_data['prediction'] = y_pred\n",
    "test_data['actual'] = y_test\n",
    "test_data['pred_change'] = test_data['prediction'].diff()\n",
    "test_data['actual_change'] = test_data['actual'].diff()\n",
    "\n",
    "test_data['pred_direction'] = np.where(test_data['pred_change'] > 0, 'up', 'down')\n",
    "test_data['actual_direction'] = np.where(test_data['actual_change'] > 0, 'up', 'down')\n",
    "\n",
    "# Calculate the precision for up and down predictions\n",
    "true_positive_up = np.sum((test_data['pred_direction'] == 'up') & (test_data['actual_direction'] == 'up'))\n",
    "false_positive_up = np.sum((test_data['pred_direction'] == 'up') & (test_data['actual_direction'] == 'down'))\n",
    "true_positive_down = np.sum((test_data['pred_direction'] == 'down') & (test_data['actual_direction'] == 'down'))\n",
    "false_positive_down = np.sum((test_data['pred_direction'] == 'down') & (test_data['actual_direction'] == 'up'))\n",
    "\n",
    "precision_up = true_positive_up / (true_positive_up + false_positive_up)\n",
    "precision_down = true_positive_down / (true_positive_down + false_positive_down)\n",
    "\n",
    "print(f\"Precision for up predictions: {precision_up:.2f}\")\n",
    "print(f\"Precision for down predictions: {precision_down:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for up predictions with threshold 0.03: 0.12\n",
      "Precision for down predictions with threshold 0.03: 0.22\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.03\n",
    "\n",
    "# Calculate up, down, and neutral prediction accuracy with threshold\n",
    "test_data['pred_change'] = test_data['prediction'].pct_change()\n",
    "test_data['actual_change'] = test_data['actual'].pct_change()\n",
    "\n",
    "test_data['pred_direction'] = np.where(test_data['pred_change'] > threshold, 'up', \n",
    "                                       np.where(test_data['pred_change'] < -threshold, 'down', 'neutral'))\n",
    "test_data['actual_direction'] = np.where(test_data['actual_change'] > threshold, 'up', \n",
    "                                         np.where(test_data['actual_change'] < -threshold, 'down', 'neutral'))\n",
    "\n",
    "# Calculate the precision for up and down predictions\n",
    "true_positive_up = np.sum((test_data['pred_direction'] == 'up') & (test_data['actual_direction'] == 'up'))\n",
    "false_positive_up = np.sum((test_data['pred_direction'] == 'up') & (test_data['actual_direction'] != 'up'))\n",
    "true_positive_down = np.sum((test_data['pred_direction'] == 'down') & (test_data['actual_direction'] == 'down'))\n",
    "false_positive_down = np.sum((test_data['pred_direction'] == 'down') & (test_data['actual_direction'] != 'down'))\n",
    "\n",
    "precision_up = true_positive_up / (true_positive_up + false_positive_up)\n",
    "precision_down = true_positive_down / (true_positive_down + false_positive_down)\n",
    "\n",
    "print(f\"Precision for up predictions with threshold {threshold}: {precision_up:.2f}\")\n",
    "print(f\"Precision for down predictions with threshold {threshold}: {precision_down:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAINT implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAINT (Self-AttentIve INTeraction Model) is a deep learning model specifically designed for tabular data. It addresses some limitations of traditional models like gradient boosting machines and other deep learning methods when dealing with heterogeneous data types (e.g., continuous and categorical features).\n",
    "\n",
    "Here are some key aspects that make SAINT special:\n",
    "\n",
    "Self-attention mechanism: SAINT leverages the self-attention mechanism from the Transformer architecture, which has proven to be effective in handling sequential data in natural language processing. In SAINT, this mechanism is adapted to model interactions between different features in tabular data, capturing both global and local patterns.\n",
    "\n",
    "Handling heterogeneous data types: SAINT is designed to handle a mix of continuous and categorical features effectively. The self-attention mechanism allows the model to weigh the importance of different features and their interactions adaptively, making it suitable for a wide range of tabular datasets.\n",
    "\n",
    "Scalability: Unlike some other deep learning methods that struggle with large datasets or high-dimensional input spaces, SAINT can scale well with the size of the dataset and the number of features. The self-attention mechanism allows the model to focus on the most relevant information, reducing the complexity of learning interactions between features.\n",
    "\n",
    "Strong empirical performance: In various benchmark datasets, SAINT has shown to outperform existing models like TabNet and gradient boosting machines, such as XGBoost, in terms of accuracy and other performance metrics.\n",
    "\n",
    "These characteristics make SAINT an attractive choice for tabular data problems, particularly when dealing with heterogeneous data types or complex feature interactions. The model's strong performance and versatility could lead to its adoption in various domains, including finance, healthcare, and retail, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, d_model)\n",
    "        self.WV = nn.Linear(d_model, d_model)\n",
    "        self.WO = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        batch_size = queries.shape[0]\n",
    "\n",
    "        Q = self.WQ(queries)\n",
    "        K = self.WK(keys)\n",
    "        V = self.WV(values)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / self.head_dim**0.5\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_logits, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.WO(attn_output)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x2, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(x2))\n",
    "\n",
    "        x2 = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(x2))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, input_size, output_size):\n",
    "        super(SAINT, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for the training and test sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Instantiate the SAINT model\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(np.unique(y_train.numpy()))  # Assumes y_train contains all class labels\n",
    "model = SAINT(num_layers=4, d_model=64, num_heads=4, d_ff=256, dropout=0.1, input_size=input_size, output_size=output_size)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Accuracy: 0.5074\n",
      "Epoch [2/300], Accuracy: 0.5223\n",
      "Epoch [3/300], Accuracy: 0.5096\n",
      "Epoch [4/300], Accuracy: 0.5117\n",
      "Epoch [5/300], Accuracy: 0.5138\n",
      "Epoch [6/300], Accuracy: 0.5138\n",
      "Epoch [7/300], Accuracy: 0.5159\n",
      "Epoch [8/300], Accuracy: 0.5372\n",
      "Epoch [9/300], Accuracy: 0.5180\n",
      "Epoch [10/300], Accuracy: 0.5265\n",
      "Epoch [11/300], Accuracy: 0.5456\n",
      "Epoch [12/300], Accuracy: 0.5159\n",
      "Epoch [13/300], Accuracy: 0.5202\n",
      "Epoch [14/300], Accuracy: 0.5117\n",
      "Epoch [15/300], Accuracy: 0.5393\n",
      "Epoch [16/300], Accuracy: 0.5159\n",
      "Epoch [17/300], Accuracy: 0.5180\n",
      "Epoch [18/300], Accuracy: 0.5074\n",
      "Epoch [19/300], Accuracy: 0.5096\n",
      "Epoch [20/300], Accuracy: 0.5159\n",
      "Epoch [21/300], Accuracy: 0.5244\n",
      "Epoch [22/300], Accuracy: 0.5011\n",
      "Epoch [23/300], Accuracy: 0.5138\n",
      "Epoch [24/300], Accuracy: 0.5138\n",
      "Epoch [25/300], Accuracy: 0.5138\n",
      "Epoch [26/300], Accuracy: 0.5096\n",
      "Epoch [27/300], Accuracy: 0.5287\n",
      "Epoch [28/300], Accuracy: 0.4989\n",
      "Epoch [29/300], Accuracy: 0.5053\n",
      "Epoch [30/300], Accuracy: 0.5117\n",
      "Epoch [31/300], Accuracy: 0.5117\n",
      "Epoch [32/300], Accuracy: 0.5244\n",
      "Epoch [33/300], Accuracy: 0.5202\n",
      "Epoch [34/300], Accuracy: 0.5096\n",
      "Epoch [35/300], Accuracy: 0.5096\n",
      "Epoch [36/300], Accuracy: 0.5308\n",
      "Epoch [37/300], Accuracy: 0.5202\n",
      "Epoch [38/300], Accuracy: 0.5074\n",
      "Epoch [39/300], Accuracy: 0.5202\n",
      "Epoch [40/300], Accuracy: 0.5223\n",
      "Epoch [41/300], Accuracy: 0.5244\n",
      "Epoch [42/300], Accuracy: 0.5265\n",
      "Epoch [43/300], Accuracy: 0.5244\n",
      "Epoch [44/300], Accuracy: 0.5138\n",
      "Epoch [45/300], Accuracy: 0.5159\n",
      "Epoch [46/300], Accuracy: 0.5265\n",
      "Epoch [47/300], Accuracy: 0.5265\n",
      "Epoch [48/300], Accuracy: 0.5138\n",
      "Epoch [49/300], Accuracy: 0.5308\n",
      "Epoch [50/300], Accuracy: 0.5287\n",
      "Epoch [51/300], Accuracy: 0.5096\n",
      "Epoch [52/300], Accuracy: 0.5096\n",
      "Epoch [53/300], Accuracy: 0.5053\n",
      "Epoch [54/300], Accuracy: 0.5032\n",
      "Epoch [55/300], Accuracy: 0.5244\n",
      "Epoch [56/300], Accuracy: 0.5265\n",
      "Epoch [57/300], Accuracy: 0.5265\n",
      "Epoch [58/300], Accuracy: 0.5159\n",
      "Epoch [59/300], Accuracy: 0.5223\n",
      "Epoch [60/300], Accuracy: 0.5032\n",
      "Epoch [61/300], Accuracy: 0.5202\n",
      "Epoch [62/300], Accuracy: 0.5244\n",
      "Epoch [63/300], Accuracy: 0.5074\n",
      "Epoch [64/300], Accuracy: 0.5117\n",
      "Epoch [65/300], Accuracy: 0.5032\n",
      "Epoch [66/300], Accuracy: 0.5011\n",
      "Epoch [67/300], Accuracy: 0.5117\n",
      "Epoch [68/300], Accuracy: 0.5308\n",
      "Epoch [69/300], Accuracy: 0.5372\n",
      "Epoch [70/300], Accuracy: 0.5244\n",
      "Epoch [71/300], Accuracy: 0.5159\n",
      "Epoch [72/300], Accuracy: 0.5372\n",
      "Epoch [73/300], Accuracy: 0.5202\n",
      "Epoch [74/300], Accuracy: 0.5180\n",
      "Epoch [75/300], Accuracy: 0.5011\n",
      "Epoch [76/300], Accuracy: 0.5308\n",
      "Epoch [77/300], Accuracy: 0.5329\n",
      "Epoch [78/300], Accuracy: 0.5096\n",
      "Epoch [79/300], Accuracy: 0.5329\n",
      "Epoch [80/300], Accuracy: 0.5244\n",
      "Epoch [81/300], Accuracy: 0.5244\n",
      "Epoch [82/300], Accuracy: 0.5478\n",
      "Epoch [83/300], Accuracy: 0.5287\n",
      "Epoch [84/300], Accuracy: 0.5265\n",
      "Epoch [85/300], Accuracy: 0.5011\n",
      "Epoch [86/300], Accuracy: 0.5265\n",
      "Epoch [87/300], Accuracy: 0.5287\n",
      "Epoch [88/300], Accuracy: 0.5350\n",
      "Epoch [89/300], Accuracy: 0.5096\n",
      "Epoch [90/300], Accuracy: 0.5329\n",
      "Epoch [91/300], Accuracy: 0.5350\n",
      "Epoch [92/300], Accuracy: 0.5223\n",
      "Epoch [93/300], Accuracy: 0.5244\n",
      "Epoch [94/300], Accuracy: 0.5265\n",
      "Epoch [95/300], Accuracy: 0.5265\n",
      "Epoch [96/300], Accuracy: 0.5414\n",
      "Epoch [97/300], Accuracy: 0.5011\n",
      "Epoch [98/300], Accuracy: 0.5372\n",
      "Epoch [99/300], Accuracy: 0.5372\n",
      "Epoch [100/300], Accuracy: 0.5032\n",
      "Epoch [101/300], Accuracy: 0.5435\n",
      "Epoch [102/300], Accuracy: 0.5435\n",
      "Epoch [103/300], Accuracy: 0.5202\n",
      "Epoch [104/300], Accuracy: 0.5096\n",
      "Epoch [105/300], Accuracy: 0.5372\n",
      "Epoch [106/300], Accuracy: 0.5393\n",
      "Epoch [107/300], Accuracy: 0.5372\n",
      "Epoch [108/300], Accuracy: 0.5308\n",
      "Epoch [109/300], Accuracy: 0.5308\n",
      "Epoch [110/300], Accuracy: 0.5180\n",
      "Epoch [111/300], Accuracy: 0.5159\n",
      "Epoch [112/300], Accuracy: 0.5138\n",
      "Epoch [113/300], Accuracy: 0.5244\n",
      "Epoch [114/300], Accuracy: 0.5202\n",
      "Epoch [115/300], Accuracy: 0.5117\n",
      "Epoch [116/300], Accuracy: 0.5244\n",
      "Epoch [117/300], Accuracy: 0.5329\n",
      "Epoch [118/300], Accuracy: 0.5329\n",
      "Epoch [119/300], Accuracy: 0.5308\n",
      "Epoch [120/300], Accuracy: 0.5138\n",
      "Epoch [121/300], Accuracy: 0.5244\n",
      "Epoch [122/300], Accuracy: 0.5159\n",
      "Epoch [123/300], Accuracy: 0.5287\n",
      "Epoch [124/300], Accuracy: 0.5159\n",
      "Epoch [125/300], Accuracy: 0.5180\n",
      "Epoch [126/300], Accuracy: 0.5308\n",
      "Epoch [127/300], Accuracy: 0.5265\n",
      "Epoch [128/300], Accuracy: 0.4989\n",
      "Epoch [129/300], Accuracy: 0.5180\n",
      "Epoch [130/300], Accuracy: 0.5265\n",
      "Epoch [131/300], Accuracy: 0.4968\n",
      "Epoch [132/300], Accuracy: 0.5117\n",
      "Epoch [133/300], Accuracy: 0.5117\n",
      "Epoch [134/300], Accuracy: 0.5223\n",
      "Epoch [135/300], Accuracy: 0.5244\n",
      "Epoch [136/300], Accuracy: 0.5287\n",
      "Epoch [137/300], Accuracy: 0.5414\n",
      "Epoch [138/300], Accuracy: 0.5265\n",
      "Epoch [139/300], Accuracy: 0.5308\n",
      "Epoch [140/300], Accuracy: 0.5180\n",
      "Epoch [141/300], Accuracy: 0.5138\n",
      "Epoch [142/300], Accuracy: 0.5372\n",
      "Epoch [143/300], Accuracy: 0.5265\n",
      "Epoch [144/300], Accuracy: 0.5180\n",
      "Epoch [145/300], Accuracy: 0.5202\n",
      "Epoch [146/300], Accuracy: 0.5350\n",
      "Epoch [147/300], Accuracy: 0.5159\n",
      "Epoch [148/300], Accuracy: 0.5265\n",
      "Epoch [149/300], Accuracy: 0.5202\n",
      "Epoch [150/300], Accuracy: 0.5287\n",
      "Epoch [151/300], Accuracy: 0.5244\n",
      "Epoch [152/300], Accuracy: 0.5414\n",
      "Epoch [153/300], Accuracy: 0.5265\n",
      "Epoch [154/300], Accuracy: 0.5287\n",
      "Epoch [155/300], Accuracy: 0.5138\n",
      "Epoch [156/300], Accuracy: 0.5032\n",
      "Epoch [157/300], Accuracy: 0.5138\n",
      "Epoch [158/300], Accuracy: 0.5244\n",
      "Epoch [159/300], Accuracy: 0.5159\n",
      "Epoch [160/300], Accuracy: 0.5308\n",
      "Epoch [161/300], Accuracy: 0.5350\n",
      "Epoch [162/300], Accuracy: 0.5393\n",
      "Epoch [163/300], Accuracy: 0.5350\n",
      "Epoch [164/300], Accuracy: 0.5265\n",
      "Epoch [165/300], Accuracy: 0.5287\n",
      "Epoch [166/300], Accuracy: 0.5520\n",
      "Epoch [167/300], Accuracy: 0.5329\n",
      "Epoch [168/300], Accuracy: 0.5244\n",
      "Epoch [169/300], Accuracy: 0.5244\n",
      "Epoch [170/300], Accuracy: 0.5202\n",
      "Epoch [171/300], Accuracy: 0.5202\n",
      "Epoch [172/300], Accuracy: 0.5435\n",
      "Epoch [173/300], Accuracy: 0.5350\n",
      "Epoch [174/300], Accuracy: 0.5265\n",
      "Epoch [175/300], Accuracy: 0.5138\n",
      "Epoch [176/300], Accuracy: 0.5329\n",
      "Epoch [177/300], Accuracy: 0.5308\n",
      "Epoch [178/300], Accuracy: 0.5096\n",
      "Epoch [179/300], Accuracy: 0.5265\n",
      "Epoch [180/300], Accuracy: 0.5478\n",
      "Epoch [181/300], Accuracy: 0.5287\n",
      "Epoch [182/300], Accuracy: 0.5202\n",
      "Epoch [183/300], Accuracy: 0.5414\n",
      "Epoch [184/300], Accuracy: 0.5180\n",
      "Epoch [185/300], Accuracy: 0.5329\n",
      "Epoch [186/300], Accuracy: 0.5478\n",
      "Epoch [187/300], Accuracy: 0.5372\n",
      "Epoch [188/300], Accuracy: 0.5329\n",
      "Epoch [189/300], Accuracy: 0.5287\n",
      "Epoch [190/300], Accuracy: 0.5414\n",
      "Epoch [191/300], Accuracy: 0.5223\n",
      "Epoch [192/300], Accuracy: 0.5287\n",
      "Epoch [193/300], Accuracy: 0.5159\n",
      "Epoch [194/300], Accuracy: 0.5265\n",
      "Epoch [195/300], Accuracy: 0.5372\n",
      "Epoch [196/300], Accuracy: 0.5350\n",
      "Epoch [197/300], Accuracy: 0.5180\n",
      "Epoch [198/300], Accuracy: 0.5202\n",
      "Epoch [199/300], Accuracy: 0.5393\n",
      "Epoch [200/300], Accuracy: 0.5329\n",
      "Epoch [201/300], Accuracy: 0.5414\n",
      "Epoch [202/300], Accuracy: 0.5287\n",
      "Epoch [203/300], Accuracy: 0.5393\n",
      "Epoch [204/300], Accuracy: 0.5287\n",
      "Epoch [205/300], Accuracy: 0.5329\n",
      "Epoch [206/300], Accuracy: 0.5096\n",
      "Epoch [207/300], Accuracy: 0.5138\n",
      "Epoch [208/300], Accuracy: 0.5478\n",
      "Epoch [209/300], Accuracy: 0.5244\n",
      "Epoch [210/300], Accuracy: 0.5117\n",
      "Epoch [211/300], Accuracy: 0.5244\n",
      "Epoch [212/300], Accuracy: 0.5350\n",
      "Epoch [213/300], Accuracy: 0.5138\n",
      "Epoch [214/300], Accuracy: 0.5244\n",
      "Epoch [215/300], Accuracy: 0.5308\n",
      "Epoch [216/300], Accuracy: 0.5202\n",
      "Epoch [217/300], Accuracy: 0.5435\n",
      "Epoch [218/300], Accuracy: 0.5414\n",
      "Epoch [219/300], Accuracy: 0.5159\n",
      "Epoch [220/300], Accuracy: 0.5393\n",
      "Epoch [221/300], Accuracy: 0.5244\n",
      "Epoch [222/300], Accuracy: 0.5329\n",
      "Epoch [223/300], Accuracy: 0.5435\n",
      "Epoch [224/300], Accuracy: 0.5648\n",
      "Epoch [225/300], Accuracy: 0.5265\n",
      "Epoch [226/300], Accuracy: 0.5159\n",
      "Epoch [227/300], Accuracy: 0.5329\n",
      "Epoch [228/300], Accuracy: 0.5244\n",
      "Epoch [229/300], Accuracy: 0.5393\n",
      "Epoch [230/300], Accuracy: 0.5159\n",
      "Epoch [231/300], Accuracy: 0.5180\n",
      "Epoch [232/300], Accuracy: 0.5287\n",
      "Epoch [233/300], Accuracy: 0.5329\n",
      "Epoch [234/300], Accuracy: 0.5244\n",
      "Epoch [235/300], Accuracy: 0.5372\n",
      "Epoch [236/300], Accuracy: 0.5308\n",
      "Epoch [237/300], Accuracy: 0.5265\n",
      "Epoch [238/300], Accuracy: 0.5329\n",
      "Epoch [239/300], Accuracy: 0.5159\n",
      "Epoch [240/300], Accuracy: 0.5287\n",
      "Epoch [241/300], Accuracy: 0.5138\n",
      "Epoch [242/300], Accuracy: 0.5372\n",
      "Epoch [243/300], Accuracy: 0.5329\n",
      "Epoch [244/300], Accuracy: 0.5435\n",
      "Epoch [245/300], Accuracy: 0.5393\n",
      "Epoch [246/300], Accuracy: 0.5265\n",
      "Epoch [247/300], Accuracy: 0.5180\n",
      "Epoch [248/300], Accuracy: 0.5372\n",
      "Epoch [249/300], Accuracy: 0.5138\n",
      "Epoch [250/300], Accuracy: 0.5393\n",
      "Epoch [251/300], Accuracy: 0.5180\n",
      "Epoch [252/300], Accuracy: 0.5414\n",
      "Epoch [253/300], Accuracy: 0.5159\n",
      "Epoch [254/300], Accuracy: 0.5287\n",
      "Epoch [255/300], Accuracy: 0.5244\n",
      "Epoch [256/300], Accuracy: 0.5308\n",
      "Epoch [257/300], Accuracy: 0.5456\n",
      "Epoch [258/300], Accuracy: 0.5159\n",
      "Epoch [259/300], Accuracy: 0.5584\n",
      "Epoch [260/300], Accuracy: 0.5287\n",
      "Epoch [261/300], Accuracy: 0.5096\n",
      "Epoch [262/300], Accuracy: 0.5138\n",
      "Epoch [263/300], Accuracy: 0.5414\n",
      "Epoch [264/300], Accuracy: 0.5159\n",
      "Epoch [265/300], Accuracy: 0.5350\n",
      "Epoch [266/300], Accuracy: 0.5520\n",
      "Epoch [267/300], Accuracy: 0.5138\n",
      "Epoch [268/300], Accuracy: 0.5754\n",
      "Epoch [269/300], Accuracy: 0.5329\n",
      "Epoch [270/300], Accuracy: 0.5393\n",
      "Epoch [271/300], Accuracy: 0.5074\n",
      "Epoch [272/300], Accuracy: 0.5287\n",
      "Epoch [273/300], Accuracy: 0.5096\n",
      "Epoch [274/300], Accuracy: 0.5202\n",
      "Epoch [275/300], Accuracy: 0.5202\n",
      "Epoch [276/300], Accuracy: 0.5308\n",
      "Epoch [277/300], Accuracy: 0.5138\n",
      "Epoch [278/300], Accuracy: 0.5053\n",
      "Epoch [279/300], Accuracy: 0.5329\n",
      "Epoch [280/300], Accuracy: 0.5308\n",
      "Epoch [281/300], Accuracy: 0.5159\n",
      "Epoch [282/300], Accuracy: 0.5372\n",
      "Epoch [283/300], Accuracy: 0.5180\n",
      "Epoch [284/300], Accuracy: 0.5202\n",
      "Epoch [285/300], Accuracy: 0.5393\n",
      "Epoch [286/300], Accuracy: 0.5096\n",
      "Epoch [287/300], Accuracy: 0.5329\n",
      "Epoch [288/300], Accuracy: 0.5372\n",
      "Epoch [289/300], Accuracy: 0.5308\n",
      "Epoch [290/300], Accuracy: 0.5180\n",
      "Epoch [291/300], Accuracy: 0.5117\n",
      "Epoch [292/300], Accuracy: 0.5287\n",
      "Epoch [293/300], Accuracy: 0.5074\n",
      "Epoch [294/300], Accuracy: 0.5159\n",
      "Epoch [295/300], Accuracy: 0.5074\n",
      "Epoch [296/300], Accuracy: 0.5287\n",
      "Epoch [297/300], Accuracy: 0.5435\n",
      "Epoch [298/300], Accuracy: 0.5265\n",
      "Epoch [299/300], Accuracy: 0.5244\n",
      "Epoch [300/300], Accuracy: 0.5435\n"
     ]
    }
   ],
   "source": [
    "# Train the SAINT model\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAINT Precision (Long): 0.55\n",
      "SAINT Precision (Short): 0.5151515151515151\n"
     ]
    }
   ],
   "source": [
    "# Obtain probabilistic outputs\n",
    "with torch.no_grad():\n",
    "    y_proba = model(X_test).numpy()\n",
    "\n",
    "# Get the predicted classes\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "# Calculate the precision scores\n",
    "precision_long = precision_score(y_test, y_pred, pos_label=1)\n",
    "precision_short = precision_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "print(\"SAINT Precision (Long):\", precision_long)\n",
    "print(\"SAINT Precision (Short):\", precision_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_76277/2246191493.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/gv/jgw7w7x15n7_rrzr0h9r68y40000gn/T/ipykernel_76277/2246191493.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Convert the training data to PyTorch tensors if needed\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Obtain probabilistic outputs for the training set\n",
    "with torch.no_grad():\n",
    "    y_train_proba = model(X_train_tensor).numpy()\n",
    "\n",
    "# Get the predicted classes for the training set\n",
    "y_train_pred = np.argmax(y_train_proba, axis=1)\n",
    "\n",
    "# Calculate the precision scores for the training set\n",
    "precision_train_long = precision_score(y_train, y_train_pred, pos_label=1)\n",
    "precision_train_short = precision_score(y_train, y_train_pred, pos_label=0)\n",
    "\n",
    "print(\"SAINT Training Precision (Long):\", precision_train_long)\n",
    "print(\"SAINT Training Precision (Short):\", precision_train_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding early stopping for SAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAINT model with early stopping and patience\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "best_accuracy = -1\n",
    "epochs_since_best = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            epochs_since_best = 0\n",
    "            # Save the best model if needed\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# Load the best model for further evaluation or use\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTT Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(FTTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x[-1])  # Select the output corresponding to the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and instantiate the model\n",
    "num_classes = 2\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = num_classes  # Set the number of output classes based on your problem\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 4\n",
    "dim_feedforward = 128\n",
    "\n",
    "model = FTTransformer(input_dim, output_dim, d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Prepare the data\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.7065, Acc: 0.5037\n",
      "Epoch [2/500], Loss: 0.6997, Acc: 0.5079\n",
      "Epoch [3/500], Loss: 0.6973, Acc: 0.5033\n",
      "Epoch [4/500], Loss: 0.6956, Acc: 0.5070\n",
      "Epoch [5/500], Loss: 0.6964, Acc: 0.5062\n",
      "Epoch [6/500], Loss: 0.6994, Acc: 0.4909\n",
      "Epoch [7/500], Loss: 0.6944, Acc: 0.5166\n",
      "Epoch [8/500], Loss: 0.6929, Acc: 0.5182\n",
      "Epoch [9/500], Loss: 0.6934, Acc: 0.5141\n",
      "Epoch [10/500], Loss: 0.6980, Acc: 0.5012\n",
      "Epoch [11/500], Loss: 0.6981, Acc: 0.4880\n",
      "Epoch [12/500], Loss: 0.6940, Acc: 0.5132\n",
      "Epoch [13/500], Loss: 0.6938, Acc: 0.5137\n",
      "Epoch [14/500], Loss: 0.6958, Acc: 0.4888\n",
      "Epoch [15/500], Loss: 0.6931, Acc: 0.5199\n",
      "Epoch [16/500], Loss: 0.6936, Acc: 0.5132\n",
      "Epoch [17/500], Loss: 0.6956, Acc: 0.5062\n",
      "Epoch [18/500], Loss: 0.6942, Acc: 0.5103\n",
      "Epoch [19/500], Loss: 0.6949, Acc: 0.5000\n",
      "Epoch [20/500], Loss: 0.6931, Acc: 0.5219\n",
      "Epoch [21/500], Loss: 0.6937, Acc: 0.5108\n",
      "Epoch [22/500], Loss: 0.6911, Acc: 0.5298\n",
      "Epoch [23/500], Loss: 0.6943, Acc: 0.5186\n",
      "Epoch [24/500], Loss: 0.6941, Acc: 0.5182\n",
      "Epoch [25/500], Loss: 0.6938, Acc: 0.5137\n",
      "Epoch [26/500], Loss: 0.6942, Acc: 0.5070\n",
      "Epoch [27/500], Loss: 0.6934, Acc: 0.5108\n",
      "Epoch [28/500], Loss: 0.6940, Acc: 0.5145\n",
      "Epoch [29/500], Loss: 0.6954, Acc: 0.4963\n",
      "Epoch [30/500], Loss: 0.6931, Acc: 0.5190\n",
      "Epoch [31/500], Loss: 0.6939, Acc: 0.5161\n",
      "Epoch [32/500], Loss: 0.6943, Acc: 0.5091\n",
      "Epoch [33/500], Loss: 0.6928, Acc: 0.5232\n",
      "Epoch [34/500], Loss: 0.6937, Acc: 0.5141\n",
      "Epoch [35/500], Loss: 0.6936, Acc: 0.5261\n",
      "Epoch [36/500], Loss: 0.6937, Acc: 0.5087\n",
      "Epoch [37/500], Loss: 0.6931, Acc: 0.5166\n",
      "Epoch [38/500], Loss: 0.6923, Acc: 0.5219\n",
      "Epoch [39/500], Loss: 0.6929, Acc: 0.5186\n",
      "Epoch [40/500], Loss: 0.6927, Acc: 0.5087\n",
      "Epoch [41/500], Loss: 0.6948, Acc: 0.5087\n",
      "Epoch [42/500], Loss: 0.6927, Acc: 0.5186\n",
      "Epoch [43/500], Loss: 0.6934, Acc: 0.5195\n",
      "Epoch [44/500], Loss: 0.6935, Acc: 0.5145\n",
      "Epoch [45/500], Loss: 0.6945, Acc: 0.5041\n",
      "Epoch [46/500], Loss: 0.6927, Acc: 0.5058\n",
      "Epoch [47/500], Loss: 0.6935, Acc: 0.5112\n",
      "Epoch [48/500], Loss: 0.6951, Acc: 0.5091\n",
      "Epoch [49/500], Loss: 0.6924, Acc: 0.5124\n",
      "Epoch [50/500], Loss: 0.6938, Acc: 0.5157\n",
      "Epoch [51/500], Loss: 0.6920, Acc: 0.5116\n",
      "Epoch [52/500], Loss: 0.6931, Acc: 0.5025\n",
      "Epoch [53/500], Loss: 0.6936, Acc: 0.5170\n",
      "Epoch [54/500], Loss: 0.6924, Acc: 0.5190\n",
      "Epoch [55/500], Loss: 0.6928, Acc: 0.5178\n",
      "Epoch [56/500], Loss: 0.6948, Acc: 0.5046\n",
      "Epoch [57/500], Loss: 0.6931, Acc: 0.5099\n",
      "Epoch [58/500], Loss: 0.6927, Acc: 0.5211\n",
      "Epoch [59/500], Loss: 0.6926, Acc: 0.5232\n",
      "Epoch [60/500], Loss: 0.6920, Acc: 0.5269\n",
      "Epoch [61/500], Loss: 0.6937, Acc: 0.5046\n",
      "Epoch [62/500], Loss: 0.6924, Acc: 0.5128\n",
      "Epoch [63/500], Loss: 0.6925, Acc: 0.5153\n",
      "Epoch [64/500], Loss: 0.6924, Acc: 0.5099\n",
      "Epoch [65/500], Loss: 0.6926, Acc: 0.5219\n",
      "Epoch [66/500], Loss: 0.6920, Acc: 0.5112\n",
      "Epoch [67/500], Loss: 0.6916, Acc: 0.5281\n",
      "Epoch [68/500], Loss: 0.6916, Acc: 0.5257\n",
      "Epoch [69/500], Loss: 0.6938, Acc: 0.5103\n",
      "Epoch [70/500], Loss: 0.6930, Acc: 0.5116\n",
      "Epoch [71/500], Loss: 0.6922, Acc: 0.5294\n",
      "Epoch [72/500], Loss: 0.6927, Acc: 0.5182\n",
      "Epoch [73/500], Loss: 0.6940, Acc: 0.5157\n",
      "Epoch [74/500], Loss: 0.6920, Acc: 0.5199\n",
      "Epoch [75/500], Loss: 0.6919, Acc: 0.5339\n",
      "Epoch [76/500], Loss: 0.6917, Acc: 0.5174\n",
      "Epoch [77/500], Loss: 0.6928, Acc: 0.5207\n",
      "Epoch [78/500], Loss: 0.6928, Acc: 0.5257\n",
      "Epoch [79/500], Loss: 0.6930, Acc: 0.5166\n",
      "Epoch [80/500], Loss: 0.6924, Acc: 0.5178\n",
      "Epoch [81/500], Loss: 0.6916, Acc: 0.5248\n",
      "Epoch [82/500], Loss: 0.6919, Acc: 0.5178\n",
      "Epoch [83/500], Loss: 0.6925, Acc: 0.5087\n",
      "Epoch [84/500], Loss: 0.6924, Acc: 0.5261\n",
      "Epoch [85/500], Loss: 0.6934, Acc: 0.5145\n",
      "Epoch [86/500], Loss: 0.6926, Acc: 0.5186\n",
      "Epoch [87/500], Loss: 0.6920, Acc: 0.5182\n",
      "Epoch [88/500], Loss: 0.6932, Acc: 0.5224\n",
      "Epoch [89/500], Loss: 0.6919, Acc: 0.5306\n",
      "Epoch [90/500], Loss: 0.6915, Acc: 0.5273\n",
      "Epoch [91/500], Loss: 0.6924, Acc: 0.5141\n",
      "Epoch [92/500], Loss: 0.6909, Acc: 0.5265\n",
      "Epoch [93/500], Loss: 0.6924, Acc: 0.5219\n",
      "Epoch [94/500], Loss: 0.6911, Acc: 0.5302\n",
      "Epoch [95/500], Loss: 0.6916, Acc: 0.5207\n",
      "Epoch [96/500], Loss: 0.6925, Acc: 0.5211\n",
      "Epoch [97/500], Loss: 0.6915, Acc: 0.5310\n",
      "Epoch [98/500], Loss: 0.6919, Acc: 0.5207\n",
      "Epoch [99/500], Loss: 0.6911, Acc: 0.5294\n",
      "Epoch [100/500], Loss: 0.6930, Acc: 0.5037\n",
      "Epoch [101/500], Loss: 0.6923, Acc: 0.5319\n",
      "Epoch [102/500], Loss: 0.6921, Acc: 0.5161\n",
      "Epoch [103/500], Loss: 0.6924, Acc: 0.5120\n",
      "Epoch [104/500], Loss: 0.6921, Acc: 0.5182\n",
      "Epoch [105/500], Loss: 0.6933, Acc: 0.5244\n",
      "Epoch [106/500], Loss: 0.6929, Acc: 0.5186\n",
      "Epoch [107/500], Loss: 0.6919, Acc: 0.5319\n",
      "Epoch [108/500], Loss: 0.6913, Acc: 0.5327\n",
      "Epoch [109/500], Loss: 0.6919, Acc: 0.5141\n",
      "Epoch [110/500], Loss: 0.6920, Acc: 0.5381\n",
      "Epoch [111/500], Loss: 0.6919, Acc: 0.5215\n",
      "Epoch [112/500], Loss: 0.6925, Acc: 0.5149\n",
      "Epoch [113/500], Loss: 0.6916, Acc: 0.5199\n",
      "Epoch [114/500], Loss: 0.6910, Acc: 0.5224\n",
      "Epoch [115/500], Loss: 0.6934, Acc: 0.5137\n",
      "Epoch [116/500], Loss: 0.6922, Acc: 0.5207\n",
      "Epoch [117/500], Loss: 0.6921, Acc: 0.5232\n",
      "Epoch [118/500], Loss: 0.6927, Acc: 0.5166\n",
      "Epoch [119/500], Loss: 0.6919, Acc: 0.5248\n",
      "Epoch [120/500], Loss: 0.6919, Acc: 0.5224\n",
      "Epoch [121/500], Loss: 0.6925, Acc: 0.5112\n",
      "Epoch [122/500], Loss: 0.6912, Acc: 0.5310\n",
      "Epoch [123/500], Loss: 0.6920, Acc: 0.5236\n",
      "Epoch [124/500], Loss: 0.6923, Acc: 0.5277\n",
      "Epoch [125/500], Loss: 0.6921, Acc: 0.5116\n",
      "Epoch [126/500], Loss: 0.6913, Acc: 0.5170\n",
      "Epoch [127/500], Loss: 0.6917, Acc: 0.5161\n",
      "Epoch [128/500], Loss: 0.6919, Acc: 0.5228\n",
      "Epoch [129/500], Loss: 0.6918, Acc: 0.5153\n",
      "Epoch [130/500], Loss: 0.6920, Acc: 0.5240\n",
      "Epoch [131/500], Loss: 0.6911, Acc: 0.5277\n",
      "Epoch [132/500], Loss: 0.6929, Acc: 0.5145\n",
      "Epoch [133/500], Loss: 0.6922, Acc: 0.5199\n",
      "Epoch [134/500], Loss: 0.6918, Acc: 0.5252\n",
      "Epoch [135/500], Loss: 0.6916, Acc: 0.5323\n",
      "Epoch [136/500], Loss: 0.6918, Acc: 0.5224\n",
      "Epoch [137/500], Loss: 0.6915, Acc: 0.5265\n",
      "Epoch [138/500], Loss: 0.6911, Acc: 0.5132\n",
      "Epoch [139/500], Loss: 0.6929, Acc: 0.5132\n",
      "Epoch [140/500], Loss: 0.6912, Acc: 0.5269\n",
      "Epoch [141/500], Loss: 0.6904, Acc: 0.5244\n",
      "Epoch [142/500], Loss: 0.6916, Acc: 0.5273\n",
      "Epoch [143/500], Loss: 0.6913, Acc: 0.5240\n",
      "Epoch [144/500], Loss: 0.6915, Acc: 0.5389\n",
      "Epoch [145/500], Loss: 0.6916, Acc: 0.5286\n",
      "Epoch [146/500], Loss: 0.6921, Acc: 0.5228\n",
      "Epoch [147/500], Loss: 0.6912, Acc: 0.5310\n",
      "Epoch [148/500], Loss: 0.6920, Acc: 0.5228\n",
      "Epoch [149/500], Loss: 0.6910, Acc: 0.5236\n",
      "Epoch [150/500], Loss: 0.6922, Acc: 0.5294\n",
      "Epoch [151/500], Loss: 0.6918, Acc: 0.5261\n",
      "Epoch [152/500], Loss: 0.6927, Acc: 0.5190\n",
      "Epoch [153/500], Loss: 0.6910, Acc: 0.5236\n",
      "Epoch [154/500], Loss: 0.6916, Acc: 0.5306\n",
      "Epoch [155/500], Loss: 0.6915, Acc: 0.5302\n",
      "Epoch [156/500], Loss: 0.6912, Acc: 0.5306\n",
      "Epoch [157/500], Loss: 0.6934, Acc: 0.5112\n",
      "Epoch [158/500], Loss: 0.6908, Acc: 0.5327\n",
      "Epoch [159/500], Loss: 0.6912, Acc: 0.5244\n",
      "Epoch [160/500], Loss: 0.6913, Acc: 0.5352\n",
      "Epoch [161/500], Loss: 0.6912, Acc: 0.5248\n",
      "Epoch [162/500], Loss: 0.6920, Acc: 0.5257\n",
      "Epoch [163/500], Loss: 0.6923, Acc: 0.5190\n",
      "Epoch [164/500], Loss: 0.6917, Acc: 0.5219\n",
      "Epoch [165/500], Loss: 0.6915, Acc: 0.5298\n",
      "Epoch [166/500], Loss: 0.6913, Acc: 0.5236\n",
      "Epoch [167/500], Loss: 0.6915, Acc: 0.5248\n",
      "Epoch [168/500], Loss: 0.6909, Acc: 0.5377\n",
      "Epoch [169/500], Loss: 0.6917, Acc: 0.5269\n",
      "Epoch [170/500], Loss: 0.6925, Acc: 0.5224\n",
      "Epoch [171/500], Loss: 0.6916, Acc: 0.5327\n",
      "Epoch [172/500], Loss: 0.6925, Acc: 0.5224\n",
      "Epoch [173/500], Loss: 0.6918, Acc: 0.5360\n",
      "Epoch [174/500], Loss: 0.6913, Acc: 0.5319\n",
      "Epoch [175/500], Loss: 0.6924, Acc: 0.5203\n",
      "Epoch [176/500], Loss: 0.6916, Acc: 0.5389\n",
      "Epoch [177/500], Loss: 0.6921, Acc: 0.5286\n",
      "Epoch [178/500], Loss: 0.6915, Acc: 0.5302\n",
      "Epoch [179/500], Loss: 0.6919, Acc: 0.5286\n",
      "Epoch [180/500], Loss: 0.6916, Acc: 0.5302\n",
      "Epoch [181/500], Loss: 0.6914, Acc: 0.5310\n",
      "Epoch [182/500], Loss: 0.6912, Acc: 0.5290\n",
      "Epoch [183/500], Loss: 0.6919, Acc: 0.5248\n",
      "Epoch [184/500], Loss: 0.6924, Acc: 0.5232\n",
      "Epoch [185/500], Loss: 0.6913, Acc: 0.5195\n",
      "Epoch [186/500], Loss: 0.6915, Acc: 0.5240\n",
      "Epoch [187/500], Loss: 0.6918, Acc: 0.5261\n",
      "Epoch [188/500], Loss: 0.6917, Acc: 0.5248\n",
      "Epoch [189/500], Loss: 0.6917, Acc: 0.5335\n",
      "Epoch [190/500], Loss: 0.6910, Acc: 0.5348\n",
      "Epoch [191/500], Loss: 0.6913, Acc: 0.5290\n",
      "Epoch [192/500], Loss: 0.6921, Acc: 0.5273\n",
      "Epoch [193/500], Loss: 0.6917, Acc: 0.5277\n",
      "Epoch [194/500], Loss: 0.6913, Acc: 0.5294\n",
      "Epoch [195/500], Loss: 0.6915, Acc: 0.5310\n",
      "Epoch [196/500], Loss: 0.6911, Acc: 0.5261\n",
      "Epoch [197/500], Loss: 0.6911, Acc: 0.5153\n",
      "Epoch [198/500], Loss: 0.6920, Acc: 0.5277\n",
      "Epoch [199/500], Loss: 0.6908, Acc: 0.5294\n",
      "Epoch [200/500], Loss: 0.6911, Acc: 0.5265\n",
      "Epoch [201/500], Loss: 0.6912, Acc: 0.5310\n",
      "Epoch [202/500], Loss: 0.6908, Acc: 0.5368\n",
      "Epoch [203/500], Loss: 0.6914, Acc: 0.5348\n",
      "Epoch [204/500], Loss: 0.6917, Acc: 0.5265\n",
      "Epoch [205/500], Loss: 0.6918, Acc: 0.5273\n",
      "Epoch [206/500], Loss: 0.6918, Acc: 0.5290\n",
      "Epoch [207/500], Loss: 0.6912, Acc: 0.5294\n",
      "Epoch [208/500], Loss: 0.6907, Acc: 0.5348\n",
      "Epoch [209/500], Loss: 0.6907, Acc: 0.5385\n",
      "Epoch [210/500], Loss: 0.6912, Acc: 0.5310\n",
      "Epoch [211/500], Loss: 0.6915, Acc: 0.5277\n",
      "Epoch [212/500], Loss: 0.6911, Acc: 0.5352\n",
      "Epoch [213/500], Loss: 0.6923, Acc: 0.5236\n",
      "Epoch [214/500], Loss: 0.6906, Acc: 0.5319\n",
      "Epoch [215/500], Loss: 0.6907, Acc: 0.5339\n",
      "Epoch [216/500], Loss: 0.6906, Acc: 0.5290\n",
      "Epoch [217/500], Loss: 0.6915, Acc: 0.5331\n",
      "Epoch [218/500], Loss: 0.6912, Acc: 0.5373\n",
      "Epoch [219/500], Loss: 0.6915, Acc: 0.5360\n",
      "Epoch [220/500], Loss: 0.6914, Acc: 0.5348\n",
      "Epoch [221/500], Loss: 0.6920, Acc: 0.5273\n",
      "Epoch [222/500], Loss: 0.6913, Acc: 0.5430\n",
      "Epoch [223/500], Loss: 0.6904, Acc: 0.5356\n",
      "Epoch [224/500], Loss: 0.6907, Acc: 0.5331\n",
      "Epoch [225/500], Loss: 0.6911, Acc: 0.5418\n",
      "Epoch [226/500], Loss: 0.6908, Acc: 0.5352\n",
      "Epoch [227/500], Loss: 0.6916, Acc: 0.5236\n",
      "Epoch [228/500], Loss: 0.6912, Acc: 0.5339\n",
      "Epoch [229/500], Loss: 0.6909, Acc: 0.5352\n",
      "Epoch [230/500], Loss: 0.6908, Acc: 0.5426\n",
      "Epoch [231/500], Loss: 0.6915, Acc: 0.5339\n",
      "Epoch [232/500], Loss: 0.6919, Acc: 0.5203\n",
      "Epoch [233/500], Loss: 0.6908, Acc: 0.5352\n",
      "Epoch [234/500], Loss: 0.6913, Acc: 0.5286\n",
      "Epoch [235/500], Loss: 0.6909, Acc: 0.5389\n",
      "Epoch [236/500], Loss: 0.6912, Acc: 0.5393\n",
      "Epoch [237/500], Loss: 0.6910, Acc: 0.5377\n",
      "Epoch [238/500], Loss: 0.6914, Acc: 0.5302\n",
      "Epoch [239/500], Loss: 0.6903, Acc: 0.5335\n",
      "Epoch [240/500], Loss: 0.6913, Acc: 0.5418\n",
      "Epoch [241/500], Loss: 0.6912, Acc: 0.5373\n",
      "Epoch [242/500], Loss: 0.6912, Acc: 0.5360\n",
      "Epoch [243/500], Loss: 0.6910, Acc: 0.5406\n",
      "Epoch [244/500], Loss: 0.6909, Acc: 0.5323\n",
      "Epoch [245/500], Loss: 0.6912, Acc: 0.5323\n",
      "Epoch [246/500], Loss: 0.6905, Acc: 0.5323\n",
      "Epoch [247/500], Loss: 0.6914, Acc: 0.5368\n",
      "Epoch [248/500], Loss: 0.6914, Acc: 0.5257\n",
      "Epoch [249/500], Loss: 0.6915, Acc: 0.5298\n",
      "Epoch [250/500], Loss: 0.6908, Acc: 0.5393\n",
      "Epoch [251/500], Loss: 0.6910, Acc: 0.5339\n",
      "Epoch [252/500], Loss: 0.6910, Acc: 0.5360\n",
      "Epoch [253/500], Loss: 0.6908, Acc: 0.5373\n",
      "Epoch [254/500], Loss: 0.6910, Acc: 0.5286\n",
      "Epoch [255/500], Loss: 0.6900, Acc: 0.5397\n",
      "Epoch [256/500], Loss: 0.6921, Acc: 0.5269\n",
      "Epoch [257/500], Loss: 0.6904, Acc: 0.5331\n",
      "Epoch [258/500], Loss: 0.6912, Acc: 0.5281\n",
      "Epoch [259/500], Loss: 0.6905, Acc: 0.5381\n",
      "Epoch [260/500], Loss: 0.6907, Acc: 0.5368\n",
      "Epoch [261/500], Loss: 0.6911, Acc: 0.5339\n",
      "Epoch [262/500], Loss: 0.6910, Acc: 0.5356\n",
      "Epoch [263/500], Loss: 0.6904, Acc: 0.5364\n",
      "Epoch [264/500], Loss: 0.6905, Acc: 0.5418\n",
      "Epoch [265/500], Loss: 0.6910, Acc: 0.5339\n",
      "Epoch [266/500], Loss: 0.6912, Acc: 0.5430\n",
      "Epoch [267/500], Loss: 0.6905, Acc: 0.5443\n",
      "Epoch [268/500], Loss: 0.6907, Acc: 0.5364\n",
      "Epoch [269/500], Loss: 0.6913, Acc: 0.5356\n",
      "Epoch [270/500], Loss: 0.6909, Acc: 0.5352\n",
      "Epoch [271/500], Loss: 0.6909, Acc: 0.5385\n",
      "Epoch [272/500], Loss: 0.6903, Acc: 0.5381\n",
      "Epoch [273/500], Loss: 0.6911, Acc: 0.5360\n",
      "Epoch [274/500], Loss: 0.6915, Acc: 0.5339\n",
      "Epoch [275/500], Loss: 0.6907, Acc: 0.5368\n",
      "Epoch [276/500], Loss: 0.6906, Acc: 0.5381\n",
      "Epoch [277/500], Loss: 0.6907, Acc: 0.5344\n",
      "Epoch [278/500], Loss: 0.6909, Acc: 0.5385\n",
      "Epoch [279/500], Loss: 0.6908, Acc: 0.5406\n",
      "Epoch [280/500], Loss: 0.6903, Acc: 0.5447\n",
      "Epoch [281/500], Loss: 0.6906, Acc: 0.5344\n",
      "Epoch [282/500], Loss: 0.6907, Acc: 0.5344\n",
      "Epoch [283/500], Loss: 0.6904, Acc: 0.5439\n",
      "Epoch [284/500], Loss: 0.6908, Acc: 0.5323\n",
      "Epoch [285/500], Loss: 0.6908, Acc: 0.5389\n",
      "Epoch [286/500], Loss: 0.6900, Acc: 0.5406\n",
      "Epoch [287/500], Loss: 0.6912, Acc: 0.5302\n",
      "Epoch [288/500], Loss: 0.6901, Acc: 0.5335\n",
      "Epoch [289/500], Loss: 0.6902, Acc: 0.5348\n",
      "Epoch [290/500], Loss: 0.6912, Acc: 0.5323\n",
      "Epoch [291/500], Loss: 0.6906, Acc: 0.5410\n",
      "Epoch [292/500], Loss: 0.6907, Acc: 0.5397\n",
      "Epoch [293/500], Loss: 0.6900, Acc: 0.5459\n",
      "Epoch [294/500], Loss: 0.6910, Acc: 0.5368\n",
      "Epoch [295/500], Loss: 0.6910, Acc: 0.5385\n",
      "Epoch [296/500], Loss: 0.6904, Acc: 0.5356\n",
      "Epoch [297/500], Loss: 0.6902, Acc: 0.5435\n",
      "Epoch [298/500], Loss: 0.6904, Acc: 0.5331\n",
      "Epoch [299/500], Loss: 0.6907, Acc: 0.5377\n",
      "Epoch [300/500], Loss: 0.6907, Acc: 0.5389\n",
      "Epoch [301/500], Loss: 0.6907, Acc: 0.5377\n",
      "Epoch [302/500], Loss: 0.6902, Acc: 0.5410\n",
      "Epoch [303/500], Loss: 0.6904, Acc: 0.5360\n",
      "Epoch [304/500], Loss: 0.6904, Acc: 0.5360\n",
      "Epoch [305/500], Loss: 0.6903, Acc: 0.5430\n",
      "Epoch [306/500], Loss: 0.6910, Acc: 0.5273\n",
      "Epoch [307/500], Loss: 0.6905, Acc: 0.5389\n",
      "Epoch [308/500], Loss: 0.6907, Acc: 0.5339\n",
      "Epoch [309/500], Loss: 0.6902, Acc: 0.5393\n",
      "Epoch [310/500], Loss: 0.6902, Acc: 0.5430\n",
      "Epoch [311/500], Loss: 0.6906, Acc: 0.5364\n",
      "Epoch [312/500], Loss: 0.6913, Acc: 0.5331\n",
      "Epoch [313/500], Loss: 0.6904, Acc: 0.5397\n",
      "Epoch [314/500], Loss: 0.6903, Acc: 0.5426\n",
      "Epoch [315/500], Loss: 0.6901, Acc: 0.5414\n",
      "Epoch [316/500], Loss: 0.6905, Acc: 0.5327\n",
      "Epoch [317/500], Loss: 0.6904, Acc: 0.5381\n",
      "Epoch [318/500], Loss: 0.6898, Acc: 0.5435\n",
      "Epoch [319/500], Loss: 0.6902, Acc: 0.5360\n",
      "Epoch [320/500], Loss: 0.6906, Acc: 0.5368\n",
      "Epoch [321/500], Loss: 0.6903, Acc: 0.5410\n",
      "Epoch [322/500], Loss: 0.6919, Acc: 0.5252\n",
      "Epoch [323/500], Loss: 0.6912, Acc: 0.5323\n",
      "Epoch [324/500], Loss: 0.6909, Acc: 0.5352\n",
      "Epoch [325/500], Loss: 0.6901, Acc: 0.5422\n",
      "Epoch [326/500], Loss: 0.6905, Acc: 0.5406\n",
      "Epoch [327/500], Loss: 0.6902, Acc: 0.5422\n",
      "Epoch [328/500], Loss: 0.6906, Acc: 0.5393\n",
      "Epoch [329/500], Loss: 0.6913, Acc: 0.5298\n",
      "Epoch [330/500], Loss: 0.6901, Acc: 0.5360\n",
      "Epoch [331/500], Loss: 0.6903, Acc: 0.5377\n",
      "Epoch [332/500], Loss: 0.6919, Acc: 0.5228\n",
      "Epoch [333/500], Loss: 0.6906, Acc: 0.5356\n",
      "Epoch [334/500], Loss: 0.6909, Acc: 0.5364\n",
      "Epoch [335/500], Loss: 0.6905, Acc: 0.5306\n",
      "Epoch [336/500], Loss: 0.6900, Acc: 0.5406\n",
      "Epoch [337/500], Loss: 0.6902, Acc: 0.5397\n",
      "Epoch [338/500], Loss: 0.6906, Acc: 0.5344\n",
      "Epoch [339/500], Loss: 0.6902, Acc: 0.5348\n",
      "Epoch [340/500], Loss: 0.6901, Acc: 0.5360\n",
      "Epoch [341/500], Loss: 0.6898, Acc: 0.5439\n",
      "Epoch [342/500], Loss: 0.6900, Acc: 0.5389\n",
      "Epoch [343/500], Loss: 0.6912, Acc: 0.5315\n",
      "Epoch [344/500], Loss: 0.6898, Acc: 0.5414\n",
      "Epoch [345/500], Loss: 0.6895, Acc: 0.5435\n",
      "Epoch [346/500], Loss: 0.6897, Acc: 0.5348\n",
      "Epoch [347/500], Loss: 0.6896, Acc: 0.5472\n",
      "Epoch [348/500], Loss: 0.6903, Acc: 0.5435\n",
      "Epoch [349/500], Loss: 0.6906, Acc: 0.5401\n",
      "Epoch [350/500], Loss: 0.6901, Acc: 0.5389\n",
      "Epoch [351/500], Loss: 0.6907, Acc: 0.5368\n",
      "Epoch [352/500], Loss: 0.6901, Acc: 0.5410\n",
      "Epoch [353/500], Loss: 0.6904, Acc: 0.5331\n",
      "Epoch [354/500], Loss: 0.6900, Acc: 0.5464\n",
      "Epoch [355/500], Loss: 0.6903, Acc: 0.5344\n",
      "Epoch [356/500], Loss: 0.6900, Acc: 0.5331\n",
      "Epoch [357/500], Loss: 0.6903, Acc: 0.5377\n",
      "Epoch [358/500], Loss: 0.6899, Acc: 0.5414\n",
      "Epoch [359/500], Loss: 0.6899, Acc: 0.5422\n",
      "Epoch [360/500], Loss: 0.6897, Acc: 0.5385\n",
      "Epoch [361/500], Loss: 0.6907, Acc: 0.5381\n",
      "Epoch [362/500], Loss: 0.6893, Acc: 0.5422\n",
      "Epoch [363/500], Loss: 0.6909, Acc: 0.5335\n",
      "Epoch [364/500], Loss: 0.6895, Acc: 0.5381\n",
      "Epoch [365/500], Loss: 0.6893, Acc: 0.5410\n",
      "Epoch [366/500], Loss: 0.6892, Acc: 0.5443\n",
      "Epoch [367/500], Loss: 0.6907, Acc: 0.5401\n",
      "Epoch [368/500], Loss: 0.6896, Acc: 0.5435\n",
      "Epoch [369/500], Loss: 0.6898, Acc: 0.5339\n",
      "Epoch [370/500], Loss: 0.6894, Acc: 0.5381\n",
      "Epoch [371/500], Loss: 0.6901, Acc: 0.5348\n",
      "Epoch [372/500], Loss: 0.6904, Acc: 0.5414\n",
      "Epoch [373/500], Loss: 0.6900, Acc: 0.5451\n",
      "Epoch [374/500], Loss: 0.6902, Acc: 0.5443\n",
      "Epoch [375/500], Loss: 0.6900, Acc: 0.5459\n",
      "Epoch [376/500], Loss: 0.6899, Acc: 0.5414\n",
      "Epoch [377/500], Loss: 0.6890, Acc: 0.5414\n",
      "Epoch [378/500], Loss: 0.6901, Acc: 0.5439\n",
      "Epoch [379/500], Loss: 0.6905, Acc: 0.5352\n",
      "Epoch [380/500], Loss: 0.6897, Acc: 0.5426\n",
      "Epoch [381/500], Loss: 0.6896, Acc: 0.5426\n",
      "Epoch [382/500], Loss: 0.6902, Acc: 0.5410\n",
      "Epoch [383/500], Loss: 0.6896, Acc: 0.5430\n",
      "Epoch [384/500], Loss: 0.6902, Acc: 0.5435\n",
      "Epoch [385/500], Loss: 0.6895, Acc: 0.5426\n",
      "Epoch [386/500], Loss: 0.6896, Acc: 0.5414\n",
      "Epoch [387/500], Loss: 0.6894, Acc: 0.5447\n",
      "Epoch [388/500], Loss: 0.6896, Acc: 0.5381\n",
      "Epoch [389/500], Loss: 0.6894, Acc: 0.5389\n",
      "Epoch [390/500], Loss: 0.6895, Acc: 0.5430\n",
      "Epoch [391/500], Loss: 0.6900, Acc: 0.5410\n",
      "Epoch [392/500], Loss: 0.6895, Acc: 0.5401\n",
      "Epoch [393/500], Loss: 0.6888, Acc: 0.5464\n",
      "Epoch [394/500], Loss: 0.6883, Acc: 0.5468\n",
      "Epoch [395/500], Loss: 0.6911, Acc: 0.5319\n",
      "Epoch [396/500], Loss: 0.6922, Acc: 0.5186\n",
      "Epoch [397/500], Loss: 0.6900, Acc: 0.5426\n",
      "Epoch [398/500], Loss: 0.6908, Acc: 0.5232\n",
      "Epoch [399/500], Loss: 0.6887, Acc: 0.5459\n",
      "Epoch [400/500], Loss: 0.6901, Acc: 0.5414\n",
      "Epoch [401/500], Loss: 0.6895, Acc: 0.5401\n",
      "Epoch [402/500], Loss: 0.6900, Acc: 0.5335\n",
      "Epoch [403/500], Loss: 0.6891, Acc: 0.5373\n",
      "Epoch [404/500], Loss: 0.6893, Acc: 0.5389\n",
      "Epoch [405/500], Loss: 0.6885, Acc: 0.5447\n",
      "Epoch [406/500], Loss: 0.6884, Acc: 0.5447\n",
      "Epoch [407/500], Loss: 0.6890, Acc: 0.5414\n",
      "Epoch [408/500], Loss: 0.6888, Acc: 0.5430\n",
      "Epoch [409/500], Loss: 0.6888, Acc: 0.5426\n",
      "Epoch [410/500], Loss: 0.6884, Acc: 0.5484\n",
      "Epoch [411/500], Loss: 0.6896, Acc: 0.5410\n",
      "Epoch [412/500], Loss: 0.6895, Acc: 0.5422\n",
      "Epoch [413/500], Loss: 0.6897, Acc: 0.5360\n",
      "Epoch [414/500], Loss: 0.6892, Acc: 0.5439\n",
      "Epoch [415/500], Loss: 0.6905, Acc: 0.5344\n",
      "Epoch [416/500], Loss: 0.6901, Acc: 0.5356\n",
      "Epoch [417/500], Loss: 0.6877, Acc: 0.5480\n",
      "Epoch [418/500], Loss: 0.6892, Acc: 0.5401\n",
      "Epoch [419/500], Loss: 0.6912, Acc: 0.5323\n",
      "Epoch [420/500], Loss: 0.6895, Acc: 0.5422\n",
      "Epoch [421/500], Loss: 0.6894, Acc: 0.5410\n",
      "Epoch [422/500], Loss: 0.6893, Acc: 0.5497\n",
      "Epoch [423/500], Loss: 0.6889, Acc: 0.5493\n",
      "Epoch [424/500], Loss: 0.6886, Acc: 0.5410\n",
      "Epoch [425/500], Loss: 0.6887, Acc: 0.5484\n",
      "Epoch [426/500], Loss: 0.6885, Acc: 0.5459\n",
      "Epoch [427/500], Loss: 0.6894, Acc: 0.5443\n",
      "Epoch [428/500], Loss: 0.6892, Acc: 0.5435\n",
      "Epoch [429/500], Loss: 0.6901, Acc: 0.5422\n",
      "Epoch [430/500], Loss: 0.6892, Acc: 0.5414\n",
      "Epoch [431/500], Loss: 0.6896, Acc: 0.5464\n",
      "Epoch [432/500], Loss: 0.6886, Acc: 0.5459\n",
      "Epoch [433/500], Loss: 0.6889, Acc: 0.5468\n",
      "Epoch [434/500], Loss: 0.6894, Acc: 0.5435\n",
      "Epoch [435/500], Loss: 0.6880, Acc: 0.5426\n",
      "Epoch [436/500], Loss: 0.6889, Acc: 0.5443\n",
      "Epoch [437/500], Loss: 0.6901, Acc: 0.5397\n",
      "Epoch [438/500], Loss: 0.6887, Acc: 0.5459\n",
      "Epoch [439/500], Loss: 0.6892, Acc: 0.5422\n",
      "Epoch [440/500], Loss: 0.6877, Acc: 0.5513\n",
      "Epoch [441/500], Loss: 0.6916, Acc: 0.5298\n",
      "Epoch [442/500], Loss: 0.6897, Acc: 0.5373\n",
      "Epoch [443/500], Loss: 0.6893, Acc: 0.5410\n",
      "Epoch [444/500], Loss: 0.6893, Acc: 0.5451\n",
      "Epoch [445/500], Loss: 0.6884, Acc: 0.5480\n",
      "Epoch [446/500], Loss: 0.6887, Acc: 0.5422\n",
      "Epoch [447/500], Loss: 0.6891, Acc: 0.5401\n",
      "Epoch [448/500], Loss: 0.6896, Acc: 0.5426\n",
      "Epoch [449/500], Loss: 0.6885, Acc: 0.5451\n",
      "Epoch [450/500], Loss: 0.6893, Acc: 0.5401\n",
      "Epoch [451/500], Loss: 0.6894, Acc: 0.5397\n",
      "Epoch [452/500], Loss: 0.6882, Acc: 0.5480\n",
      "Epoch [453/500], Loss: 0.6885, Acc: 0.5464\n",
      "Epoch [454/500], Loss: 0.6896, Acc: 0.5426\n",
      "Epoch [455/500], Loss: 0.6895, Acc: 0.5377\n",
      "Epoch [456/500], Loss: 0.6881, Acc: 0.5472\n",
      "Epoch [457/500], Loss: 0.6888, Acc: 0.5426\n",
      "Epoch [458/500], Loss: 0.6885, Acc: 0.5480\n",
      "Epoch [459/500], Loss: 0.6876, Acc: 0.5435\n",
      "Epoch [460/500], Loss: 0.6902, Acc: 0.5373\n",
      "Epoch [461/500], Loss: 0.6884, Acc: 0.5401\n",
      "Epoch [462/500], Loss: 0.6878, Acc: 0.5451\n",
      "Epoch [463/500], Loss: 0.6900, Acc: 0.5422\n",
      "Epoch [464/500], Loss: 0.6891, Acc: 0.5509\n",
      "Epoch [465/500], Loss: 0.6883, Acc: 0.5459\n",
      "Epoch [466/500], Loss: 0.6891, Acc: 0.5422\n",
      "Epoch [467/500], Loss: 0.6874, Acc: 0.5472\n",
      "Epoch [468/500], Loss: 0.6886, Acc: 0.5389\n",
      "Epoch [469/500], Loss: 0.6899, Acc: 0.5414\n",
      "Epoch [470/500], Loss: 0.6891, Acc: 0.5422\n",
      "Epoch [471/500], Loss: 0.6877, Acc: 0.5464\n",
      "Epoch [472/500], Loss: 0.6887, Acc: 0.5447\n",
      "Epoch [473/500], Loss: 0.6884, Acc: 0.5488\n",
      "Epoch [474/500], Loss: 0.6886, Acc: 0.5406\n",
      "Epoch [475/500], Loss: 0.6886, Acc: 0.5439\n",
      "Epoch [476/500], Loss: 0.6880, Acc: 0.5468\n",
      "Epoch [477/500], Loss: 0.6924, Acc: 0.5315\n",
      "Epoch [478/500], Loss: 0.6908, Acc: 0.5261\n",
      "Epoch [479/500], Loss: 0.6893, Acc: 0.5356\n",
      "Epoch [480/500], Loss: 0.6898, Acc: 0.5373\n",
      "Epoch [481/500], Loss: 0.6887, Acc: 0.5389\n",
      "Epoch [482/500], Loss: 0.6884, Acc: 0.5406\n",
      "Epoch [483/500], Loss: 0.6900, Acc: 0.5464\n",
      "Epoch [484/500], Loss: 0.6885, Acc: 0.5418\n",
      "Epoch [485/500], Loss: 0.6897, Acc: 0.5430\n",
      "Epoch [486/500], Loss: 0.6901, Acc: 0.5422\n",
      "Epoch [487/500], Loss: 0.6877, Acc: 0.5476\n",
      "Epoch [488/500], Loss: 0.6893, Acc: 0.5406\n",
      "Epoch [489/500], Loss: 0.6885, Acc: 0.5468\n",
      "Epoch [490/500], Loss: 0.6890, Acc: 0.5501\n",
      "Epoch [491/500], Loss: 0.6908, Acc: 0.5339\n",
      "Epoch [492/500], Loss: 0.6896, Acc: 0.5377\n",
      "Epoch [493/500], Loss: 0.6890, Acc: 0.5435\n",
      "Epoch [494/500], Loss: 0.6888, Acc: 0.5435\n",
      "Epoch [495/500], Loss: 0.6884, Acc: 0.5451\n",
      "Epoch [496/500], Loss: 0.6882, Acc: 0.5435\n",
      "Epoch [497/500], Loss: 0.6886, Acc: 0.5439\n",
      "Epoch [498/500], Loss: 0.6873, Acc: 0.5468\n",
      "Epoch [499/500], Loss: 0.6934, Acc: 0.5141\n",
      "Epoch [500/500], Loss: 0.6925, Acc: 0.5211\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and correct predictions\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "    # Calculate the epoch's loss and accuracy\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "\n",
    "    # Print the epoch's loss and accuracy\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5298, Test Precision: 0.5274\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_corrects = 0\n",
    "test_total_samples = 0\n",
    "test_true_positives = 0\n",
    "test_false_positives = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_running_corrects += torch.sum(preds == labels).item()\n",
    "        test_total_samples += inputs.size(0)\n",
    "        test_true_positives += torch.sum((preds == 1) & (labels == 1)).item()\n",
    "        test_false_positives += torch.sum((preds == 1) & (labels == 0)).item()\n",
    "\n",
    "test_acc = test_running_corrects / test_total_samples\n",
    "test_precision = test_true_positives / (test_true_positives + test_false_positives)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Precision: {test_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cqm-primo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfeab4218defce68a1a28795986f1baaaa2315ecb6579607c6e60cd9359d5b1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
